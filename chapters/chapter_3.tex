\section{Introduction}
\label{implementation_testing_introduction}

In this chapter we will present the results of the implementation and validation process used for developing the predictive model described in chapter \ref{chapter_theory_modelling}. Indeed, fundamental to our approach for approximating the latent motivational state of an individual is to have a model able to reliably predict the intensity of future behaviour (i.e. future engagement in a videogame context) given the history of interactions between an individual and a potential rewarding object (i.e. a videogame). To achieve this, we adopted a variation of bottom-up iterative model building \cite{gelman2020bayesian} in which first the simplest version of a model is designed, built and evaluated and then, based on performance and addition theoretical assumptions, a new improved version of the same model is proposed. At each stage of the process we evaluate the new version of the model against alternative approaches in order to test for hypotheses stated in the model design stage. Each section of this chapter corresponds to a different version of the model and will have the following structure. In \textbf{Model Design} we state the task the model is trying to solve, the design of such model and the theoretical assumptions that informed the design. In \textbf{Data} we describe the dataset used for evaluating the performance of our model along with any data-related processing. In \textbf{Model Comparison} we outline the alternative approaches against which the current model is compared and the various procedures and statistical analyses employed. In \textbf{Results} we report the outcomes of the model comparison procedure with particular focus on the assessment of the predictive accuracy. In \textbf{Model Criticism} we discuss what presented in the results section,in light of the theoretical assumptions used when designing the model, and highlight potential improvements to be carried out in the subsequent stage of the model building process. Despite the \textbf{Model Comparison} stage differed slightly between the various stages of the model building process, a common experimental pipeline was adopted. We can see a graphical representation of the latter in Figure \ref{fig: pipeline_eval}
\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{images/chapter_3/pipeline_eval.png}
    \caption[\textbf{Model implementation experimental pipeline}]{Arrows indicate the flow of the pipeline. Big coloured blocks are major pipeline steps, white rectangles indicate sub-tasks within each step.}
    \label{fig: pipeline_eval}
\end{figure}
\section{Joint Prediction of Long Term Behavioural Intensity}
\label{model_architecture_1}
In the first iteration of our model building process we focused on implementing and testing some of the core theoretical assumptions presented in chapter \ref{chapter_theory_modelling}, namely the importance of modelling temporal and non-linear dynamics in the interactions between an individual and a videogame. The experimental task used for this purpose aimed at predicting the long term intensity of interactions between an individual and a videogame given a set of behavioural metrics recorded during an initial observation period. Here, the observation period is defined as a small sequence of initial interactions between an individual and a videogame. In this specific context, we used churn and survival time as behavioural proxies for the expected intensity of future interactions. We briefly introduced these two concepts in section \ref{engagement_prediction}. More formally we can say that given a sequence of interactions with associated behavioural intensity $B_{t_1 : T}$, survival time is the amount of expected future playing activity at a given point $t_n$ \cite{ perianez2016churn, demediuk2018player, bertens2017games, kim2017churn, viljanen2018playtime}
\begin{equation}
\label{eq_survival}
    survival = \sum_{t_n}^{T}B_t
\end{equation}
while churn is the probability of observing a terminal event $C$ (usually formulated as a prolonged period of inactivity from the individual) after a given point $t_n$ \cite{hadiji2014predicting,runge2014churn, drachen2016rapid,milovsevic2017early, kim2017churn}.
\begin{equation}
\label{eq_churn}
    churn = \sum_{t_n}^{T}C_n = 1
\end{equation}
here the interactions going from $t_1: t_n$ define our observation period. It has to be noted that this task can be viewed as a special case of the more general formulation presented in section \ref{td_to_supervised}, precisely 
\begin{equation}
\label{rnn_1_exp}
   \mathbb{E}[B_{t_n : T}] = \mathbb{E}[B_{t_n : T}]
\end{equation}


and correspond to predicting extinction and future ammount of sustained engagement after observing a handful of interactions following the point of engagement (according to O'Brien et al. \cite{o2008user} framework presented in section \ref{eng_reward_motivation}). Given this experimental context, the hypotheses that we wanted to evaluate in order to support our modelling intuitions were the following:
\begin{enumerate}
    \item Leveraging all the information present in the observation period is required for achieving higher predictive power.
    \item Approaches able to account for non-linear interactions in the considered behavioural metrics can outperform simpler additive models.
    \item The ability to model the type of sequential dynamics presented in section \ref{td_to_supervised} can have a positive effect on the predictive performance.
    \item It is possible to have a single model jointly predict the considered metrics of long term behavioural intensity without any loss of predictive accuracy.
    \item It is possible to have a single model performing the predictive task simultaneously across a wide range of games.
\end{enumerate}

\subsection{Model Design}
\label{model_design_1}
The first iteration of our ANN architecture, loosely inspired by the winning entry in \cite{lee2018game}, aimed to jointly predict survival time and churn probability across a set of 6 different game contexts. This architecture, the `Bifurcating Model' (BM), is illustrated in Fig. \ref{fig: rnn_1}. 
\input{diagrams/chapter_3/rnn_31}
The model receives as input a set of variable length multivariate time series composed of 5 metrics described in section \ref{data_1}, resulting in an $B \in \mathbb{R}^{N \times T \times 5}$ tensor where $N$ is the number of time series and $T$ is the length of the longest series. All the series shorter then $T$ are made of the same length by adding a padding value $pad=9999$ at the end (this value will never appear in the dataset). An additional set of univariate time series of shape $O \in \mathbb{N}^{N \times 1 \times 1}$ is used for indicating to which game context the behavioural metrics belongs to.  It has to be noted that these series contain numerical encoding of the game context (e.g. $jc3 \mapsto 1$, $lis \mapsto 2$ etc.) in order to be able to represent the associated information through a so called embedding operation (see Appendix \ref{embedding_operation}). Given the input series, the operation will generate an $O^* \in \mathbb{R}^{N \times T \times h}$  tensor with $h$ being the number of hidden units chosen for the embedding. This can be thought as a form of over-parametrization that allows each single context to have a non-sparse representation and to be projected into a multi-dimensional space where the relationships between elements become meaningful (e.g. game contexts which are similar to each other in respect to the objectives will be located closer to each other in the embedding space). This, other than allowing each context to carry more information, provides a general representation that grows richer and richer the more categories are included into it. Obviously this would require to re-fit the model whenever a new unseen context is added as this approach does not support out-of-sample generalizability. It is important to highlight that this embedding operation is fundamental to the construction of a global model as outlined in section \ref{manifold_learning}. Indeed, it allows to appropriately associate parameters to all the game contexts taken in consideration and to include them in a single model. Next, the raw behavioural input and the embedded game context vector are concatenated along the temporal dimension producing a single $OB \in \mathbb{R}^{N \times T \times h + 5}$ tensor and a masking operation is applied. This operation allows the model to skip computations whenever the $pad$ value is encountered \cite{chollet2015keras}. These newly obtained features are then modelled across time using a recurrent neural network (RNN) with Long Short-Term Memory (LSTM) operations (see Appendix \ref{lstm_operation}). In this specific setting the RNN is of type many-to-one \cite{bengio2017deep} therefore producing an $H \in \mathbb{R}^{N \times h}$ matrix (with $h$ being the number of hidden units chosen for the recurrent operation). As we already highlighted in section \ref{td_to_supervised} the use of an RNN in this context is of pivotal importance for modelling the sequential dynamics underlying the process of incentive salience attribution. Indeed, if we think of the representation generated by the RNN in terms of expectation
\begin{equation}
\label{rnn_1_exp}
   \mathbb{E}[B_{t_n : T}] = f(h_{t_n}; \theta)
\end{equation}
what the LSTM operation allows us to do is to estimate $h_{t_n}$ as the result of a process with memory where the intensity of previous interactions with a specific game determines the expected intensity of all future interactions. The final step of this architecture uses the latent representation generated by the RNN for producing predictions for survival time and churn. This is accomplished by means of multi-task learning with two ANNs with fully connected operation (see Appendix \ref{fnn_operation}) receiving the matrix generated by the RNN as input and producing the predictions for the relative behavioural targets in the form of a vector of shape $\hat{y} \in \mathbb{R}^{N}$. The model used $ReLU$ as activation function (see Appendix \ref{relu}) for the hidden units whereas the link functions used for generating churn and survival time predictions were the $identity$ (see Appendix \ref{identity}) and $sigmoid$ (see Appendix \ref{sigmoid}) functions. We applied two regularization techniques after each fully connected operation, namely: batch normalization \cite{ioffe2015batch} and dropout \cite{srivastava2014dropout} (see Appendix \ref{dropout} and \ref{batch_norm}).

\paragraph*{Competing Models}
\label{competing_models_1}
In order to test the hypotheses mentioned in section \ref{model_architecture_1} we implemented a series of competing models for disjoint estimation of survival time and churn probability. Furthermore, we decided to include a baseline model (MM) which generates predictions based on the average of the targets in the training set. The presence of a baseline which doesn't require any form of parameters estimation provides a sanity check on the overall quality of the models and complexity of the problem at hands (e.g. if a  predictive task is trivial to solve we expect a relatively satisfactory level of accuracy even by a naive baseline model). The competing models were chosen according to two main criteria: the ability to capture linear and non-linear interactions between features and most importantly the capability to fit large data-sets (e.g. matrix of dimension $x \in \mathbb{R}^{\approx10^6\times10^2}$). According to this criteria we opted for linear regression with ElasticNet regularization (see appendix \ref{enet_reg}) (EN) \cite{zou2005regularization}, logistic regression (LR) and two Multi-Layer Perceptrons using  $id$ (MLPr) and $sigma$ (MLPc) as link functions. Multi-Layer Perceptrons are ANNs using the type of feedforward operation described in Appendix \ref{fnn_operation}. We felt that given the similarities between linear models and ANNs, which can be seen as a stacked version of the former but with more `expressive power', the chosen algorithms constituted a natural progression in the modelling approach. 

\subsection{Data}
\label{data_1}
In order to evaluate the performance of the different models in the experimental predictive task, we needed to acquire records of interactions between individuals and potentially rewarding objects in naturalistic contexts. As mentioned in section \ref{videogame_telemetries}, video games are particularly suited for this purpose given their learning-dependent reinforcing properties and the large amount of longitudinal data streams that they can generate. For this reason we gathered data from six different games published by our partner company, \textit{Square Enix Ltd}. Focusing on maintaining heterogeneity in genre and platform, we considered the following titles: \emph{Hitman Go} (hmg), \emph{Hitman Sniper} (hms), \emph{Just Cause 3} (jc3), \emph{Just Cause 4} (jc4), \emph{Life is Strange} (lis), and \emph{Life is Strange: Before the Storm} (lisbf). A general description of each of these titles can be found in Table \ref{game_description_31}. Due to the diversity in their in-game mechanics, each of these games was considered as an "object" with different reinforcing properties (see section \ref{videogame_telemetries}). This allowed us to mimic a situation where a single model had access to data coming from a heterogeneous set of potentially rewarding entities (similarly to what we described in section \ref{motivation_hist}). Data were gathered from any user playing between the game's release and February 2019, allowing us to adopt more robust sampling strategies which for each game utilizes the breadth of virtually the entire user-base. To rule out possible `faulty' but not `naturally abnormal' data, we restricted the data cleaning process to a single filter applied at query time to ignore users having at least one of the considered metric over the game population's \nth{99} percentile. This allowed us to make little assumptions on the distribution of the data as well as providing a convenient stress test for eventual future applications.

\input{tables/chapter_3/descriptive_statistics_31}

\paragraph*{Defining the Observation Period}
Because we were interested in predicting survival time and churn probability based only on a restricted number of early individual-game interactions it was important to define a cut-off for what would be considered an "early sequence of interactions" (the so called observation period (OP)). Choosing the length for the OP was not a trivial task as there is little indication in the literature about optimal cut-off values. Hence, we decided to visually inspect the data a-priori and extend rules proposed in \cite{drachen2016rapid, milovsevic2017early} to take into account natural inter-individual differences. Therefore, we defined the cut-off as:

\begin{equation}
\label{cutoffop}
    \text{cutoff} = 
    \Biggl\lceil
        \dfrac
            {min(T, completion_t)}
            {3}
    \Biggr\rceil
\end{equation}

Where $T$ is the total number of game play sessions and $completion_t$ is the number of game play sessions before the user completed the game for the first time. In this way we take the first $\frac{1}{3}$ of all played sessions for players who churned and the first $\frac{1}{3}$ of played sessions before a non-churning player completed the game for the first time. We apply this cut off to the ordered list of all recorded play sessions for a specific user. We decided to use game sessions as the temporal dimension, rather than total minutes played, since we believed it better adjusted for each user's "pace" (i.e. not all the users have the possibility to play at the same frequency). Since the length of the OP has a naturally different distribution between the churning and non-churning population, we stratified our sampling technique to maintain a similar ratio of OP lengths among churners and non churners. This becomes particularly relevant when employing models that leverage the entire sequence of interactions where the length of the OP could leak information about the considered targets (e.g. churn is on average associated with a small number of interactions). Summarizing, if an individual had 9 total sessions recorded, we considered the first 3 for making predictions on what happened from the $4^{th}$ to the $9^{th}$ session. 

\paragraph*{Defining the Behavioural Metrics and Targets}
\label{behavioural_metric_targets_1}
We considered a set of 5 metrics, easily generalizable across games and comparable with metrics employed in other behavioural studies of incentive salience attribution \cite{berridge1998role,mcclure2003computational,zhang2009neural}, and retrieved them temporally  (i.e. over each game session during the OP), see Table \ref{metricsdescription_1} for a description. Additionally, we acquired a single context feature specifying the game context from where the metrics were originated. For determining the targets for our survival and churn estimation tasks, we leveraged existing literature on churn prediction \cite{drachen2016rapid, milovsevic2017early, lee2018game, perianez2016churn, runge2014churn, kim2017churn, hadiji2014predicting, xie2015predicting} and survival analysis \cite{viljanen2018playtime, demediuk2018player, lee2018game, bertens2017games}, extending existing rules to accommodate the need to define churn and survival time in single player games with a defined life cycle (i.e. non Games-as-a-Service GaaS). Indeed, while GaaS can only rely on inactivity periods for determining churn, titles with a defined life cycle (e.g. single player games) can utilize a defined end-game period as a hard cut-off for distinguishing between churners and non-churners: users finishing a game are not churners even if they stop playing afterwards. In this view, we took advantage of having access to the complete session history for all users to create a churn definition which was robust to the variance in play patterns across games by taking into account all the recorded inter-session distances. The criteria we adopted for defining a user as churner were both: 

\begin{enumerate}
    \item Not completing the game
    \item Being inactive for a period equal or greater to:
        \begin{equation}
            \label{inactivityrule}
            \text{inactivity} = 
            mean(\mathbf{x}) + 2.5 \cdot std(\mathbf{x})
        \end{equation}
\end{enumerate}

For better adjusting for inter-individual differences, we could have applied formula \ref{inactivityrule} to each user individually but this could have created accuracy issue for individuals with very few recorded sessions. Therefore, we opted for a conservative but more robust approach applying inactivity ($\mathbf{x}$) $\forall \mathbf{x} \in X$ where $X$ is the collection of all the considered games and $\mathbf{x}$ is the vector of inter-sessions distances in minutes for a specific game. The use of formula \ref{inactivityrule} allowed us to estimate an inactivity period which was not arbitrarily chosen but statistically defined as "extraordinary long" in accordance with characteristics of play patterns in a particular game. For defining the survival time, we simply computed the total amount of play time in minutes for a user minus the amount of play time during the OP.

\input{tables/chapter_3/features_target_description_31}

\paragraph*{Data Preparation}
\label{data_preparation_1}
We adopted specific data preparation for testing the various hypotheses specified in section \ref{model_architecture_1}. We first generated a dataset collapsing the original data over the temporal dimension retrieving mean and standard deviation of each considered metric and adding a one-hot encoded (see Appendix \ref{one_hot_encode_operation} version of the game context indicator. Then we generated a second data-frame maintaining the original temporal structure and numerically encoding the game context indicators. As we mentioned before, different users had OP of different lengths so we applied the $pad$ value to each metric column in the data-set so to have each sequence of considered sessions to the length of the longest sequence in the data-set. For each experiment we created a tuning and test subsets (i.e. 20 and 80 \% of the original data-set) via stratified shuffle split \cite{scikit-learn}, employing the first for hyper-parameters tuning and the second for model evaluation. Each time a model was fit on the considered dataset, we re-scaled each metric separately for each game in an outliers-robust way:
\begin{equation}
\label{robustscaler}
    \text{RobustRescale}=
        \dfrac
            {\mathbf{x} - Q_2(\mathbf{x})}
            {Q_3(\mathbf{x}) - Q_1(\mathbf{x})}
\end{equation}
where $\mathbf{x}$ is the feature vector to be re-scaled and $Q_n$ is the $n^{th}$ quartile. 

\subsection{Model Tuning and Comparison}
\label{tuning_comparison_1}
For all the considered models we first, determined the best hyper-parameters via simple grid search 10-fold stratified cross validation \cite{scikit-learn} on the tuning set. At each iteration of the 10-fold stratified cross validation a small sub-set of the fold used for fitting the model (i.e. 10\%) was set aside and used to evaluate model convergence and eventually trigger an early-stopping policy (so to avoid over-fitting). For all the models convergence was determined by the loss not improving for at least 3 consecutive epochs. The models were trained though mini-batch stochastic gradient descent (using a batch size of 256) using the Adaptive Moment Estimation (ADAM) optimizer \cite{kingma2014adam} with learning rate adjusted through a cyclical policy \cite{smith2017cyclical, chollet2015keras}. We used Mean Squared Error (MSE) and Binary Cross Entropy (BCE) respectively (see Appendix \ref{mse} and \ref{bce}) as objective functions for the disjoint models (i.e. linear models and MLPs). Differently from the other models the objective function for the the BM model was given by the unweighted sum of the losses associated with the two branching ANNs, namely BCE for the churn prediction task and the Symmetric Mean Absolute Percentage Error (SMAPE) (see Appendix \ref{smape}) for the survival prediction task. The SMAPE is bounded between 0 and 100 and can be interpreted as percentage deviation from the target with lower values indicating better model fit. The choice of SMAPE was motivated by the fact that its scale invariance allowed better comparisons of results across game contexts. For EN the best hyper-parameters were $\lambda = 0.1$ and $alpha=0.5$ for the regularization term regularization. For LR a $\lambda = 0.01$ for the lasso regularization was found to yield the best results. Both MLPr and MLPc employed an Ridge regularization with $\lambda=0.01$ and utilized a 3 layers architecture with 200, 100 and 50 hidden units. The BM architecture used an embedding with 40 hidden units, an LSTM RNN with 100 hidden units and two fully connected ANNs with 300 hidden units each. The best dropout rate was found to be $p=0.1$ meaning that at each forward pass 1 in ten units for the fully connected ANNs were set to 0. Once the best hyper-parameters were found, each model was evaluated on the the test by fitting it on 90\% of the set and performing out-of-sample prediction for the remaining 10\%. All the disjoint models were tested on both the collapsed and unfolded datasets while the BM model only on the last one. This is because the BM model was specifically designed for performing predictions using data in a time series format. Moreover, following the intuition from \cite{gal2016dropout}, the BM model supported Montecarlo dropout: a method for approximating Bayesian inference and producing posterior distribution of the model's predictions. This was achieved by querying the model 50 times at prediction time and retaining all the produced values. When computing the performance metrics we then used the mean of the estimated values, since they roughly followed a normal distribution. For the churn prediction task the chosen metric was the macro-averaged F1 score (see Appendix \ref{F1}) (i.e. employing the unweighted mean of precision and recall for both classes) while for the survival time prediction task the chosen metric was the SMAPE. The code for tuning and evaluating the models was written in Python 3.6, with the algorithms provided by the library scikit-learn \cite{scikit-learn} and  Keras (with Tensorflow as a back-end) \cite{chollet2015keras}.

\subsection{Results}
\label{results_1}
We will first present results for each disjoint model as well as for the baseline model. Next we will illustrate in detail the performance of the BM model both in terms of its raw accuracy and its capability to include uncertainty in its predictions. Note that for all reported SMAPE results the smaller the better as it represents the error between the prediction and ground truth. Conversely, for F1 the larger the better since it measures the discriminate power of the models on a classification task. The probability threshold employed for discriminating between classes was set to $p=0.5$. We want to highlight that, given our chosen model evaluation strategy, all the results presented here report the expected value (i.e. the mean) of a scoring metric over a single hold-out test hence conventional sample-based statistical analysis are not feasible. However, in order to quantify the uncertainty in the computed evaluation metrics we reported the standard error of the mean along with the expected value. For example in the case of SMAPE we can see from the formula in Appendix \ref{smape} that the metric computes the empirical mean over the Symmetric Percentage Error (SAPE), in order to obtain a measure of uncertainty of this estimate is sufficient to compute $SE_{SAPE} = \frac{\sigma_{SAPE}}{N}$ with $\sigma_{SAPE}$ being the standard deviation of the SAPE computed over the entire test set $N$. Moreover in order to gather a general sense of global performance, in Figure \ref{model_comp_coll_31} for each model we visualized the expected performance collapsing on game context.
\begin{figure*}[h]
\centering
\includegraphics[width=.8\textwidth]{images/chapter_3/global_31.png}
\caption[\textbf{Aggregated comparison of models' performance}]{The BM architecture outperformed all competing ones on both target using however more parameters and computation time. Bar-plots in the first column show the average performance across the 6 game contexts along with estimated standard errors of the mean. The bar-plots on the second and third columns express the number of parameters and the convergence time for each model.}
\label{model_comp_coll_31} 
\end{figure*}
\input{tables/chapter_3/mean_model_performance}
The results from the predictive task carried out using the collapsed dataset, Table \ref{collapsedperformance}, showed how all the 4 models strongly outperformed the MM baseline, Table \ref{baseperformance}, in all games, while also achieving an overall satisfying performance. Moreover we noticed how MLPr and MLPc markedly outperformed EN and LR in both churn probability and survival time prediction across all games.  
\input{tables/chapter_3/collapsed_models_performance}
Looking at the same modelling approaches on the unfolded version of the same dataset we can observed a similar pattern of results, see Table \ref{unfoldedperformance}, regarding baseline and inter-models comparisons. However, it was clear that using unfolded, temporal data lead to only small improvements over the aggregated data. This might be explained by the fact that the chosen modelling approaches are not explicitly designed for taking temporal structure into account, for example they have no explicit mechanics for temporal modelling such as those provided by a LSTM.
\input{tables/chapter_3/unfolded_models_performance}
Indeed, when evaluating the performance of the BM, Table \ref{bifurcatingperformance}, on the unfolded data we can see how the model achieves consistent improvements in both churn probability and survival time prediction in all game contexts compared to the previous best model (MLPr and MLPc). From a visual inspection of Figure \ref{perfsurv} 
\begin{figure}[h]
  \centering
  \includegraphics[width=8.5cm]{images/chapter_3/performance_survival_31.pdf}
  \caption[\textbf{Performance of the BM on survival task}]{The scatter plot shows the relationship between the survival time prediction provided by the BM and the ground truth values. Since the relationship is evaluated on the $log$ of both variables, due to the presence extreme outliers in the ground truth, this acts as mostly as a qualitative complement to the more reliable SMAPE measure.}
  \label{perfsurv}
\end{figure}
we can see the presence of a positive linear relationship between estimated and ground truth survival time (indicative of accordance between the two), with a roughly even distribution of error along the entire range of values. In Table \ref{confusionmatrix} \input{tables/chapter_3/confusion_31} 
we can observe how the model performance is evenly split across the two classes highlighting similar levels of precision and recall. Finally, observing the density plots in Figure \ref{fig:densurv} and \ref{fig:denchurn}
\begin{figure}[h]
  \centering
  \subfloat[Survival Predictions]{\includegraphics[ width=8.5cm]{images/chapter_3/survival_density_31.pdf}\label{fig:densurv}}
  \hfill
  \subfloat[Churn Probability Predictions]{\includegraphics[ width=8.5cm]{images/chapter_3/churn_density_31.pdf}\label{fig:denchurn}}
  \caption[\textbf{Distribution of the BM predictions for six random users, one for each game}]{For better comparison the survival estimates are re-scaled game-wise in the range 0 to 1. The highest density point in the distribution represents the most probable predicted value (i.e. the actual prediction),  while the area under the curve instead can be seen as measure of uncertainty (i.e. how confident is the model in its prediction).}
  \label{distestimations}
\end{figure}
we can see how the model was able to encode different levels of uncertainty in the predicted values through different levels of posterior variance.
\input{tables/chapter_3/bifurcating_model_performance}

\subsection{Model Criticism}
\label{model_criticims_1}
The different level of performance achieved by the baseline model highlighted how different game context proved to be a more challenging ground (e.g. $jc4$ and $lisbf$) than others with respect to the predictive task. Although we did not perform an exhaustive test for evaluating if disjoint models (i.e. models fitted separately to each game context) would outperform our joint modelling approach, we noticed how each parametric model (which all adopt a joint modelling approach) outperformed the baseline(which adopts a disjoint modelling approach) while maintaining a similar performance distribution across game contexts. By looking at the performance of the various parametric models we saw how  the use of models able to capture non-linear interactions between features provided substantial improvements in predicting measures of future behavioural intensity. We also showed that considering the entire history of individual-game interactions provided a consistent but small edge over metrics representations which are collapsed over time. However, this improvement appeared to be markedly more pronounced and consistent when the predictive task was carried out by non-linear models able to take the dynamical nature of the data into account (e.g. the proposed BM architecture). Finally, by looking more in details at the performance of the the BM highlighted how the proposed methodology generalizes well when trying to predict survival time and churn probability as well as successfully incorporating measures of uncertainty in its estimations. In summary, the BM architecture designed following the theoretical principles outlined in chapter \ref{chapter_theory_modelling} showed improved predictive accuracy when compared to alternatives with comparable computational expressiveness (i.e. the MLP architectures). That said, it is evident that the architecture doesn't come fully equipped with all the constrains outlined in chapter \ref{chapter_theory_modelling}
which are necessary for obtaining a good approximation of the type of construct (i.e. attributed incentive salience) presented in the works of McClure et al. \cite{mcclure2003computational} and Zhang et al. \cite{zhang2009neural}. First of all the BM architecture aims at predicting the cumulative expected ammount of future behaviour at an arbitrary early point in time $t \ll T$ (the OP defined in section \ref{model_design_1}) . This implies that, despite the latent representation generated at time $t$ possesses similar predictive powers to attributed incentive salience, these are not on the same time scale and most importantly are not defined for all the $t \in T$. Second despite churn probability and survival time can be considered good approximations of future behavioural intensity, they surely do not fully capture the complexity of the phenomenon. Moreover, these two metrics can be considered of second order (i.e. they are derived from raw quantifiers of behaviour intensity) and artificially created for serving specific concrete applications in the context of engagement quantification. Since the aim of this thesis is to estimate motivation related latent states (from which, as we said in chapter \ref{chapter_lit_review}, the behavioural manifestation of engagement can be directly derived) the next iteration in our model building process focused on modifying the BM architecture in order to make its functional form closer to the specifications highlighted in chapter \ref{chapter_theory_modelling}.

\section{Dynamic Prediction of Future Behavioural Intensity}
\label{model_architecture_2}
In the second iteration of our model building process we focused on improving and expanding the BM architecture in order to increase its flexibility and ability to incorporate the functional constrains specified in chapter \ref{chapter_theory_modelling}. Two major constrains were introduced:
\begin{enumerate}
    \item The new architecture had to jointly predict 5 behavioural metrics. These, differently from churn and survival time, were first order indicator of future behavioural intensity similar to those find in the behavioural neuroscience literature \cite{schultz1997neural,mcclure2003computational,berridge2004motivation,zhang2009neural}
    \item We moved the predictive task from a "static" (i.e. predicting a long-term target after observing a fixed sequence of individual-game interaction) to a "dynamic" one (i.e. performing prediction after each new interaction is observed).
\end{enumerate}
In line with these constrains the experimental task used in this second stage of the model building process aimed at predicting the behavioural intensity of the next interaction between an individual and a videogame after observing an arbitrary number of interactions. More formally, given a sequence of interactions with associated behavioural intensity $B_{t_1 : T}$, the target of the model at any given point in time $t_n$ became
\begin{equation}
\label{joint_target_eq}
   B_{t_{n+1}} = \{B^1_{t_{n+1}}, B^2_{t_{n+1}}, B^3_{t_{n+1}}, B^4_{t_{n+1}}, B^5_{t_{n+1}}\}
\end{equation}
with $\{B^1_{t_{n+1}}, B^2_{t_{n+1}}, B^3_{t_{n+1}}, B^4_{t_{n+1}}, B^5_{t_{n+1}}\}$ being the lead-1 version of the behavioural inputs provided to the model (more details are provided in sections \ref{model_design_2} and \ref{data_2}. This new task is now in line with the formulation presented in section \ref{td_to_supervised} and correspond to predicting the behavioural manifestations of all the phases of the engagement process model \cite{o2008user}, however in a dynamic fashion. The major aim of this second experimental task was to validate the results obtained during the evaluation of the BM architecture while submitting to model constrains that were consistent with the theoretical framework presented in chapter \ref{chapter_lit_review} and \ref{chapter_theory_modelling}. In this view, the hypotheses that we wanted to validate were tightly connected with those presented in section \ref{model_architecture_1}, and had the following form:
\begin{enumerate}
    \item Approaches able to account for non-linear interactions in the considered behavioural metrics can outperform simpler additive models.
    \item The ability to model the type of sequential dynamics presented in section \ref{td_to_supervised} can have a positive effect on the predictive performance.
    \item It is possible to have a single model jointly multiple first-order metrics of behavioural intensity without any substantial loss in accuracy
    \item It is possible to have a single model performing the predictive task simultaneously across a wide range of games.
\end{enumerate}

\subsection{Model Design}
\label{model_design_2}
The second iteration of our ANN architecture was a direct descendant of the BM architecture and aimed to jointly predict a set of 5 behavioural metrics all indicative of the intensity of interactions between an individual and a videogame. Despite being markedly different, the RNN architecture shares similarities with technique used in the neuroscience literature for inferring latent states able to predict observable behaviours \cite{calhoun2019unsupervised}. For example Calhoun et al. used a combination of Hidden Markov Model (HMM) and Generalized Linear Model (GLM) for analyzing the natural behaviour of flies \cite{calhoun2019unsupervised}. Similarly to our proposed architecture, the HMM-GLM approach first infer a latent representation of the observed behaviour though a dynamical model (i.e. HMM) and then uses the same representation for performing predictions using a set of non-linear additive model (i.e. GLM). Our architecture, that we call recurrent (RNN) for simplicity, illustrated in Figure \ref{fig: rnn_2}
\input{diagrams/chapter_3/rnn_32}
extend the work of Calhoun et al. by removing the additive and markovian assumptions and by dropping the requirement for a fixed number of hidden states. The model receives the same input of the architecture described in section \ref{model_design_2} with the major difference that no padding was required. Indeed we decided to exploit the ability of RNN to flexibly fit variable-length time series and opted for a data generator approach \footnote{See \cite{chollet2015keras,tensorflow2015-whitepaper} for implementation details.}, feeding data to the models in random batches with constant length within a batch. The type of operations performed by the architecture were identical to those presented in section \ref{model_design_1} although with 2 major differences. First, the RNN component of the architecture was now of type many-to-many therefore producing an $H \in \mathbb{R}^{N \times T \times h}$ tensor constituting the sequence of hidden states produced by the LSTM operation after observing each element of the input time series\cite{bengio2017deep}. Second, the final step of the architecture now exploits the temporal nature of the latent representation generated by the RNN for predicting not just a single value but a sequence of values, namely the lead-1 version of the input time series. This was again achieved by means of multi-task learning with five ANNs with fully connected operations. In order to apply the fully connected operations to a sequence of latent states we distributed their computations across time \footnote{See \cite{chollet2015keras} for implementation details} (see Appendix \ref{time_distributed}) so that each ANN could produce a tensor of shape $\hat{y} \in \mathbb{R}^{N \times T \times 1}$. We want to highlight that the model now fully reflect the formulation presented in section \ref{td_to_supervised}, indeed if again we look at the representations generated by the LSTM in terms of target expectation
\begin{gather}
\label{rnn_1_exp}
   h_t = f^1(O, B_{t}, h_{t-1}; \theta^1)  \\ \nonumber
   \mathbb{E}[B_{t+1}] = f^2(h_t; \theta^2) \\ \nonumber    
\end{gather}
we can can see how at each point in time the generated representation $h_t$ is mostly indicative of the behavioural intensity a time $t+1$ but it also needs to retain some predictive power for all the subsequent time steps. The model used a combination of $ReLU$, $ELU$ and $LReLU$ (see Appendix \ref{elu} and \ref{lelu}) as activations functions for the hidden units (more details on their selection are specified in \ref{tuning_comparison_2} whereas the link function used for generating the prediction was always the $identity$ function. As a regularization strategy we decided to drop the use of batch normalization, as it proved challenging to adopt with a multivariate time-series target, and introduces a variant of dropout (i.e. One Dimensional Spatial Dropout see Appendix \ref{spatial_dropout}) more suitable for time series data and applied it after each layer in the architecture.

\paragraph{Competing Models}
\label{competing_models_2}
As in the previous stage of the model building process a series of competing models were implemented for testing our hypotheses. First, we decided to improve our baseline by establishing two reasonable single-parameter models. The first is a Lag 1 model producing predictions according to the following rule:
\begin{equation}
   \begin{gathered}  
     B_{t+1} = B_{t}
     \label{lag_1}
  \end{gathered}
\end{equation}
here $t$ represent a single game session in a sequence of $T$ observed interactions while $B$ are the same behavioural metrics the RNN model recives as input (they are thoroughly described in section \ref{data_2}). The second is a Median model computing the expectation of each of the 5 targets according to the formula:
\begin{equation}
  \begin{gathered}  
    \overline{B_{t+1}} = \frac
      {\sum_{i=1}^{t+1} wB_{i}}
      {\sum_{i=1}^{t+1} w }\\
    B_{t+1} = median(\overline{B_{t+1}}) 
    \label{median}
  \end{gathered}
\end{equation}
here $\overline{B_{t+1}}$ is an exponentially weighted average of all the $B_t$ up to $t+1$ observed when fitting the model. This is computed separately for every individual in the dataset and the median value of each of the 5 targets is used a a constant prediction. These apparently naive models provide a surprisingly robust prediction baseline for time series that are not white noise \cite{hyndman2018forecasting} other than having a nice interpretation in terms of behavioural momentum \cite{nevin2000behavioral}: in conditions of high experienced reward the behaviour of an individual tends to be consistent over time (i.e. resistant to change). Similarly to the model used in the assessment of the BM architecture, a multi-task linear model with Elastic-Net regularization (ENet) \cite{zou2005regularization} was used to evaluate the performance of simple additive models. While for testing the effect of non linearity, a multi-task Multilayer Perceptron (MLP) was employed. Both model used an embedding operation for encoding information from the game context but the ENet model had the number of hidden units fixed to 1 an relied on an $identity$ activation function. This was done for avoiding to introduce non-linearity in the ENet model. Figure \ref{fig: mlp_2} show the architecture used for both the ENet and MLP models. We can see that differently from the models used in section \ref{model_architecture_1} the only difference with RNN architecture lies in the use of time distributed fully connected operations instead of those provided by the LSTM. 
\input{diagrams/chapter_3/mlp_32}

\subsection{Data}
\label{data_2}
In this iteration of the model building processes we decided to expand and improve the dataset used for evaluating the perfromance in the predictive task. We again used gameplay data from the same six video games published by our partner company, \textit{Square Enix Ltd.} however this time we increased the number of considered individuals by almost 3-fold. The resulting dataset contained entries from 3,209,336 individuals, evenly distributed across the six games, and randomly sampled from all users who played the games between their respective release date and January 2020. All data were obtained and processed in compliance with the European Union's General Data Protection Regulation \cite{EUdataregulations2018}. 
\paragraph*{Defining the Behavioural Metrics and Targets}
In order to represent the behavioural manifestation of state transition dynamics (i.e. changes in the level of attributed salience during sequences of interactions between a user and a videogame), for each individual we retrieved a set of six different types telemetry over variable-length sequences of game sessions. A game session was defined from the moment an individual started the game software until it was closed. We retrieved all sessions produced by an individual from the moment the data they generated first appeared in the game's servers. Since our modelling approach required to predict, in a supervised manner, the intensity of future playing behaviour given the history of previous interactions, we only considered users with two or more observed game sessions. The reason for this is two fold: sequences of length one do not entail any temporal structure and do not allow to generate a supervised target. The telemetry (see Table \ref{metricsdescription_2}) were almost the same of those used in the validation of the BM architecture with the only exemption of the metric "Activity Diversity" which was replace by "Active Time". This was motivated by the fact that we couldn't find any reference in the literature indicating that "Activity Diversity" was a suitable descriptor of behavioural intensity. 
\input{tables/chapter_3/features_target_description_32}
We want to highlight that the high dispersion values (Inter Quartile Range  or IQR), reported for some of the telemetry are due to the extreme skewness in the distribution of the data. This is caused both by the nature of the phenomenon they describe (e.g. Absence is a classic case of time-to-event measure) and by their typical behaviour in the context of videogames \cite{bauckhage2012players}. The final dataset was composed of 6 columns and 28,155,199 rows. A table of descriptive statistics can be found in \ref{game_description_32}.
\input{tables/chapter_3/descriptive_statistics_32}
\paragraph*{Data Preparation} When querying the data from the game servers, we excluded from the random sampling procedure individuals having at least one of the considered behavioural metrics over the game population's \nth{99} percentile. This allowed us to eliminate potentially faulty data which are often present when dealing with telemetry. At this point data were re-arranged in a format suitable for time series modelling and randomly split into a tuning (i.e. 10 \%) and validation set (i.e. 90 \%). As we mentioned in section \ref{model_architecture_2}, when fitting each model we adopted a data generator approach, this was done by batching both datasets in a series of multidimensional arrays of shape $(N \times T \times 5)$ (with $T$ being the number of available game sessions within a batch and $N$ the number of individuals inside said batch) and saving them on a local machine. When fitting or performing predictions a generator would then pass the batches in random order to the model allowing it to parse time series with different lengths between batches. For the sake of clarity we report an example of how the data from a single game session are generated and how they are parsed by the models.\\
\\
\textit{
"A user decides, 36 hours after the release of game X, to enter the game world for the first time. This is when a session starts and actual playing behaviour can be observed. During this session they engage in various activities leading to 20 non-unique and user-initiated actions (e.g. being attacked by a non-playable character is not counted as a valid action). After roughly 60 minutes spent playing, the user exits the game world and the session ends. Of this session, 80\% of time has been spent actively playing, the remaining 20\% has seen the game on pause or the user away from the console (i.e. idle time). After 48 hours the user logs into the game world again and a new session starts"}\\
\\
What we described here would correspond to a single time step $t_{1}$ in a sequence of $T$ total interactions (i.e. sessions) between a user and the specific game context X. The models will parse this session as a vector of length 4 with values 36, 20, 60, 80 and 20 along with another vector of length 1 containing the numerical index for the game. When all the sessions are observed the models will receive as inputs sequences of length $T$ of the same vectors. The behavioural metrics were min-max scaled according to the formula
\begin{equation}
  \begin{gathered} 
  \label{min_max}
        MinMax(x) =\frac{x - \min(x)} {\max(x) - \min(x)} 
  \end{gathered}
\end{equation}
where $x$ is the input vector to be scaled, while the categorical input (i.e. game object) was encoded ordinally.

\subsection{Model Tuning and Comparison}
\label{tuning_comparison_2}
Learning from the shortcoming of our previous model testing and in line with the increased complexity of considered architectures, we decided to improve our approach to hype-parameter searching and model comparison. The reason behind a more exhaustive, effective and efficient selection of hyper-parameters was motivated not just by perfromance concerns (i.e. the accuracy of ANNs can be substantially influenced by the choice of hyperparameters) but also by methodological ones. Indeed, architectural choices in ANNs can be characterized by an elevated number of degrees of freedom some of which would need to be factored out in order to perform a fair comparison between models. For example, when comparing our RNN architecture against linear or MLP one, we wanted to be able to attribute differences in performance to the introduction of non-linear and sequential operations rather than to the number of layers, hidden units or choice of activation functions. Indeed, these factors can influence the number of free parameters and expressive power of an ANN. Manually picking their optimal value is often a challenging combinatorial problem that can lead to unexpected outcomes if left to the subjective choice of the experimenter. In this view, the first step in our model comparison phase aimed to control for the contribution of hyper-parameters in the performance of the parametric models (especially for MLP and RNN) using an algorithmic approach. This was done using the Keras Tuner implementation \cite{omalley2019kerastuner} of the Hyperband algorithm \cite{li2017hyperband}. Hyperband is an optimized version of random search that achieves faster convergence through adaptive resources allocation and early termination of training. It can lead to better or equivalent results to other optimization algorithms but in a fraction of the time \cite{li2017hyperband}. When initializing the tuning step we allowed each model to grow as much as the others (except for E-Net, which,  due to the fact that it is a linear model, is naturally constrained to a fixed number of parameters) so that any observed difference in number of parameters was related to characteristics of the model architecture. The tuning step was conducted running one full iteration of Hyperband with a budget of 40 epochs \footnote{See  \cite{li2017hyperband,hyperwebs} for a detailed description of the Hyperband technique.} on the tuning set. To trigger early stopping for a specific configuration of hyperparameters, we monitored the decrease in loss over a 20\% random sample of the tuning set (we call this the validation tuning set) and we terminated training when the loss reduced by less than $\delta = 1\mathrm{e}{-4}$ for 10 consecutive epochs. Once the best set of hyperparameters was found we proceeded to fit all the models specified in section \ref{competing_models_2} on the validation set using a 10-fold Cross Validation Strategy. This  divided the validation set in 10 equally sized folds and iteratively used 9 of them for training and 1 for testing. In order to take into account the contributions of time, game and target, the performance of each model was given by computing the Symmetric Mean Percentage Error (SMAPE) \cite{zhu2017deep} for each combination of the aforementioned dimensions (e.g. SMAPE of Session Time at $t1$ for the game object hmg). Each model was trained for a maximum of 200 epochs and interrupted using the same early stopping strategy mentioned above (i.e. absence of $\delta$ reduction in loss on a 20\% hold-out for 10 consecutive epochs). The models were trained with stochastic gradient descent using the Adaptive Moment Estimation (Adam) \cite{kingma2014adam} algorithm. We decided to drop the use of a scheduling approach for specifying the optimizer learning rate as it empirically showed to not provide any substantial improvement. Instead we kept default values provided by the implementation provided by the Keras library \cite{chollet2015keras}. The optimizer was minimizing the SMAPE between the targets and the predictions generated by the model. The choice of SMAPE was dictated by the fact that the targets were expressed on largely different scales (i.e. coming from different games and expressed on different units of measure see Table \ref{metricsdescription_2}) and therefore required a loss function measuring relative distance from the target. To evaluate model perfromance we used a combination of visual inspection and inferential statistics. First, we visualized the variations in SMAPE for each combination of of target and model by collapsing the scoring metric over different dimensions (e.g. time and game context). Second, to evaluate the overall performance, we first summed the SMAPE relative to each target in a single global performance indicator: this is the loss function that each model attempts to minimize during training. We then divided the total by 5 (i.e. the total number of targets) in order to express the metric in its original scale (i.e. 0 to 100). This was then regressed using a Linear Mixed-effects Model (LMM) with game object and time as random effects and model type as fixed effect (treatment coded with the RNN architecture as reference). Subsequently, for a more thorough investigation of model performance we conducted the same regression analysis separately on each target. Both regression analyses were followed by post-hoc comparisons (i.e. t-tests with Bonferroni correction) for testing the following pairwise hypotheses on the estimated coefficients: Lag 1 $<$ Median $<$ ENet $<$ MLP. A similar type of analysis was conducted within a Bayesian framework in order to more reliably assess the perfromance of our models. No substantial discrepancies with the frequentist analyses could be found, hence details and additional results are provided in Appendix \ref{dynamic_prediction_ancillary_perf}. All statistical analyses were conducted using the python library statsmodels \cite{seabold2010statsmodels}.  All the models, except for Median, were implemented using Tensorflow's high level API "Keras" \cite{tensorflow2015-whitepaper,chollet2015keras}. The Median model was implemented using the libraries for scientific computing Pandas and Numpy \cite{reback2020pandas,harris2020array}.

\subsection{Results}
\label{results_2}
Inspecting the perfromance of the RNN architecture over the time dimension, Figure \ref{model_comp_coll_game_32}, we can see how the model not only outperformed competing approaches for most of the considered targets (only for the metric Future Absence the perfromance is comparable to the MLP architecture) but it did so over all the considered time horizons. Additionally we can see how in general the performance improved the more historical information were available to the model. A similar effect can also be observed for the MLP architecture but to a much lesser extent.
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{images/chapter_3/models_comparison_collapsed_game_32.png}
\caption[\textbf{Model comparison collapsing over game context}]{ Overall, our approach (RNN) outperforms all the competing approaches at every time horizon. Each column represent the perfromance of the considered models on a specific target. Solid lines indicate the expected \% error over time for a specific combination of target and model. Dashed areas indicate the standard error of the mean.}
\label{model_comp_coll_game_32}
\end{figure}
Similar conclusions can be drawn from inspecting the perfromance over the game-context dimension, Figure \ref{model_comp_coll_time_32}, where the RNN achieved the lowest error rate in almost every game context-target combination. In the few occasions in which this was not the case the perfromance was at least comparable with that of the MLP architecture (although always better when looking at the expected performance).
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{images/chapter_3/models_comparison_collapsed_time_32.png}
\caption[\textbf{Model comparison collapsing over time}]{ Overall, our approach (RNN) outperforms all the competing approaches in most of the target-game context combinations. Each column represent the perfromance of the considered models on a specific target. Bars indicate the expected \% error for a specific combination of game context, target and model. Black vertical lines indicate the standard error of the mean.}
\label{model_comp_coll_time_32} 
\end{figure}
These results remain almost unchanged even wen considering the interaction between game context and time (see Figure \ref{model_comp_non_coll_32}).
\begin{figure}[h]
\centering
\includegraphics[height=0.5\textheight,keepaspectratio]{images/chapter_3/models_comparison_non_collapsed_32.png}
\caption[\textbf{Model comparison without collapsing}]{ Overall, our approach (RNN) outperforms all the competing approaches in most of the target-game context combinations and temporal horizons. Each column represents the perfromance of the considered models on a specific target while each row reports the perfromance on a specific game context. Solid lines indicate the expected \% error over time for a specific combination of target and model. Dashed areas indicate the standard error of the mean.}
\label{model_comp_non_coll_32} 
\end{figure}
At the level of global performance the RNN model markedly outperformed all competing approaches as clearly shown in Figure \ref{model_comp_coll_32}. This can be also seen in the results of the regression (see Table \ref{collapsed_lmm_32}) and  \textit{post hoc} analysis (see Table \ref{collapsed_post_hoc_32}). From the \textit{post hoc} analysis we can also observe how all the pairwise hypotheses presented in paragraph \ref{tuning_comparison_2} are confirmed. Here model performance is given by the sum of all the losses produced by the five targets and therefore provides a general indicator of model fit where lower values indicate a better performance overall.
\begin{figure}[h]
\centering
\includegraphics[width=.5\columnwidth]{images/chapter_3/performance_collapsed_32.png}
\caption[\textbf{Aggregated comparison of model performance}]{ Overall, our approach (RNN) outperforms all the competing approaches. Box-plots show the 10-fold cross-validation performance expressed as the total percentage of error (i.e. SMAPE) of each model over the five targets.}
\label{model_comp_coll_32} 
\end{figure}
\input{tables/chapter_3/collapsed_lmm_performance_32}
\input{tables/chapter_3/collapsed_posthoc_performance_32}
The superiority of the RNN model can still be observed when comparing the models on each target separately. However, the size of the effect varies depending on the target (see Table \ref{exploded_lmm_32}). The same trend is also present in the \textit{post hoc} analysis (see Table \ref{exploded_post_hoc_32}) where we observe only a partial confirmation of the pairwise hypotheses. The ENet model is outperformed by the Median baseline for three targets, namely Future Active Time, Session Time and Session Activity. All the coefficients in the regression analyses and the differences in the \textit{post hoc} analyses are non-standardized and can be interpreted as absolute changes in percentage error (i.e. SMAPE). In order to make these values more easily interpretable, we can use the information Table \ref{game_description_32}. For example, knowing that the median Session Time for the jc3 object is 162 minutes we can derive that when the RNN model achieves a SMAPE of 30\% in predicting Future Session Time, this equates on average to an absolute error of $1.62 \times 30 \sim 48$ minutes. All the p-values in the \textit{post hoc} analyses are Bonferroni corrected for multiple comparisons. The results of the statistical analyses suggest positive additive effects of non-linearity and recurrency on model performance both at the level of global and target-specific performance. This effect is more pronounced for certain targets (e.g. Future Session Time, Future N° Sessions) than for others (e.g. Future Absence, Future Active Time). Moreover, looking at Figure \ref{model_comp_exp_32} it appears that RNN improved on MLP (i.e. the second best model) using roughly half the parameters and per-epoch computation time. 
\input{tables/chapter_3/exploded_lmm_performance_32}
\input{tables/chapter_3/exploded_posthoc_performance_32}
\begin{figure*}[h]
\centering
\includegraphics[width=.8\textwidth]{images/chapter_3/performance_exploded_32.png}
\caption[\textbf{Dis-aggregated comparison of models' performance}]{Our approach (RNN) outperformed all competing ones on each target. It consistently used fewer parameters and had shorter computation time than the second best performing model. Box-plots show the 10-fold cross-validation performance expressed as percentage of error (i.e. SMAPE) of each model for the five targets. The bar-plot on the top row indicates the number of free parameters for each model while the bar plot on the bottom row shows the average time for each training epoch. Both bar-plots are $log_{10}$ scaled.}
\label{model_comp_exp_32} 
\end{figure*}

\subsection{Model Criticism}
\label{model_criticims_2}
Similarly to what presented in section \ref{results_1} we were able to observe different level of perfromance for the baseline models in different game contexts, suggesting once again an heterogeneity in the level of challenge for the predictive task. A similar level of heterogeneity was also found among the considered behavioural targets (e.g. Future Absence appeared to be a much harder target to predict than Future N° of Sessions) and temporal horizons (e.g. predictions made at an early or late stage showed to be more or less challenging depending on the considered target, game context and model architecture). By looking at the performance of the various parametric models we were able to replicate our initial findings, suggesting the importance of modelling non-linear dynamics when predicting measures of future behavioural intensity. When a model provided better perfromance than a competing one it did so among all the considered game contexts and behavioural targets, again confirming that the adopted global multi-task learning approach did not cause any degradation in predictive accuracy. In summary, improving the BM model and modifying its architecture in order to better incorporate the theoretical constrains outlined in chapter \ref{chapter_theory_modelling} did not substantially change any of the findings reported in the first iteration of the model building process. That said, despite the RNN architecture was now able to generate representations compatible with the computational frameworks presented by McClure et al. \cite{mcclure2003computational} and Zhang et al. \cite{zhang2009neural} it was still not taking into account factors that might be relevant for reliably estimating the level of attributed incentive salience. Indeed, as we mentioned in chapter \ref{chapter_lit_review}, we know that this type of latent state, and its associated dynamics, are modulated by the internal condition of the individual \cite{zhang2009neural}, by the characteristics of the rewarding object (a specific videogame in this case) with which the individual is interacting and by the environment in which the interaction is occurring \cite{palminteri2015contextual}. These factors, were only partially and indirectly captured by the RNN architecture as they require more granular information (i.e. in-game and environmental information) and with a higher temporal resolution (i.e. within rather than between game sessions). As a consequence we can see how the RNN architecture, despite outperforming competing ones, still achieves a relatively high error rate in predicting some of the considered behavioural targets (e.g. future Absence). A possible solution in this case would be to incorporate information about the environment in which the observed behaviour occurred (e.g. time and location of a specific game session) and about the characteristics of the rewarding objects (i.e. the so called videogame structural characteristics that we mentioned in chapter \ref{chapter_lit_review}). As well as improving the predictive performance of the model, these new type of information should also increase the quality of the generated representation and consequently allow for a better approximation of the level of attributed incentive salience. The next iteration of the model building process will aim at modifying the RNN architecture in order to incorporate the type of environmental and game-specific information that we just mentioned.

\section{Dynamic Prediction of Future Behavioural Intensity with Environmental and Game Covariates}
\label{model_architecture_3}
\lorem

\subsection{Model Design}
\label{model_design_3}
\lorem
\input{diagrams/chapter_3/rnn_33}
\input{diagrams/chapter_3/mlp_33}

\subsection{Data}
\label{data_3}
\input{tables/chapter_3/descriptive_statistics_33}
\input{tables/chapter_3/features_target_description_33}
\lorem

\subsection{Model Tuning and Comparison}
\label{tuning_comparison_3}
\lorem

\subsection{Results}
\label{results_3}
\lorem
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{images/chapter_3/models_comparison_collapsed_game_33.png}
\caption[\textbf{Model comparison collapsing over game context}]{ Overall, our approach (RNN) outperforms all the competing approaches at every time horizon. Each column represent the perfromance of the considered models on a specific target. Solid lines indicate the expected \% error over time for a specific combination of target and model. Dashed areas indicate the standard error of the mean.}
\label{model_comp_coll_game_33}
\end{figure}
\lorem
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{images/chapter_3/models_comparison_collapsed_time_33.png}
\caption[\textbf{Model comparison collapsing over time}]{ Overall, our approach (RNN) outperforms all the competing approaches in most of the target-game context combinations. Each column represent the perfromance of the considered models on a specific target. Bars indicate the expected \% error for a specific combination of game context, target and model. Black vertical lines indicate the standard error of the mean.}
\label{model_comp_coll_time_33} 
\end{figure}
\lorem
\begin{figure}[h]
\centering
\includegraphics[height=0.5\textheight,keepaspectratio]{images/chapter_3/models_comparison_non_collapsed_33.png}
\caption[\textbf{Model comparison without collapsing}]{ Overall, our approach (RNN) outperforms all the competing approaches in most of the target-game context combinations and temporal horizons. Each column represents the perfromance of the considered models on a specific target while each row reports the perfromance on a specific game context. Solid lines indicate the expected \% error over time for a specific combination of target and model. Dashed areas indicate the standard error of the mean.}
\label{model_comp_non_coll_33} 
\end{figure}
\lorem
\begin{figure}[h]
\centering
\includegraphics[width=.5\columnwidth]{images/chapter_3/performance_collapsed_33.png}
\caption[\textbf{Aggregated comparison of model performance}]{ Overall, our approach (RNN) outperforms all the competing approaches. Box-plots show the 10-fold cross-validation performance expressed as the total percentage of error (i.e. SMAPE) of each model over the five targets.}
\label{model_comp_coll_33} 
\end{figure}
\lorem
\input{tables/chapter_3/collapsed_lmm_performance_33}
\input{tables/chapter_3/collapsed_posthoc_performance_33}
\lorem
\input{tables/chapter_3/exploded_lmm_performance_33}
\input{tables/chapter_3/exploded_posthoc_performance_33}
\begin{figure*}[h]
\centering
\includegraphics[width=.8\textwidth]{images/chapter_3/performance_exploded_33.png}
\caption[\textbf{Dis-aggregated comparison of models' performance}]{Our approach (RNN) outperformed all competing ones on each target. It consistently used fewer parameters and had shorter computation time than the second best performing model. Box-plots show the 10-fold cross-validation performance expressed as percentage of error (i.e. SMAPE) of each model for the five targets. The bar-plot on the top row indicates the number of free parameters for each model while the bar plot on the bottom row shows the average time for each training epoch. Both bar-plots are $log_{10}$ scaled.}
\label{model_comp_expl_33} 
\end{figure*}

\section{Model Criticism}
\label{model_criticism_3}
The work we just presented is not exempt from limitations. Our approach is formally different from that of TD Learning \footnote{See \cite{barto2004reinforcement} for a  review of the differences between supervised and reinforcement learning.} and does not model the process of incentive salience attribution but rather attempt to approximate the product of this process (i.e. changes in attributed incentive salience). For this reason a direct comparison with the work of McClure \textit{et. al.} \cite{mcclure2003computational} and Zhang \textit{et. al.} \cite{zhang2009neural} is difficult. Moreover, unlike TD learning \cite{schultz1997neural} our model is not guaranteed to converge on a quantification of $V$ that is directly comparable to its biological counterpart or that has arisen from the same type of computations. This is also reinforced by the differences in mechanistic functioning between biological and artificial neural networks \cite{lillicrap2019backpropagation,lillicrap2020backpropagation}. These issues are partially attenuated by the constraints provided by our theoretical framework but in line with similar reports in the literature \cite{calhoun2019unsupervised,wang2018prefrontal} a verification based on controlled experiments is desirable. Differently from the works of Calhoun \textit{et. al.} \cite{calhoun2019unsupervised},  McClure \textit{et. al.} \cite{mcclure2003computational} and Zhang \textit{et. al.} \cite{zhang2009neural}, our methodology relies on a  supervised learning approach to perform both prediction of future behaviour and latent state estimation, making this two tasks infeasible before any data is observed. This limitation could be attenuated by initializing our model using a representation  generated in an unsupervised manner. As we mentioned in section \ref{videogame_telemetries} the reward dynamics generated by the interaction between the individual and the game incentive mechanics play an important role in determining the intensity of future playing behaviour \cite{agarwal2017quitting, avserivskis2017computational, wang2018beyond}. Lastly, despite the fact that our approach appeared to deal gracefully  with objects having different structural characteristics, these were limited to the domain of video games. In order to verify the generalizability of our approach, future work should include data generated from a variety of contexts (e.g. web services, online and laboratory-based experiments).


\section{Discussion}
The advantage provided by the combination of non-linearity and recurrency in the estimation task is in line with the dynamical nature of motivation and incentive salience attribution \cite{toates1994comparing,robinson1993neural,zhang2009neural,tindell2009dynamic,berridge2012prediction}. This is also consistent with a body of research showing that the attribution of value to potentially rewarding objects or actions is often carried out by non-linear recurrent operations \cite{song2017reward,wang2018prefrontal} and that Artificial Neural Networks with recurrent connections are well suited for approximating these operations \cite{kietzmann2018deep}. These findings are corroborated not just by the superior performance of the RNN model in the prediction task (see section \ref{perf_results}) but also by its capacity to produce more stable representations (see Figure \ref{predictive_panel}).
The results of our experiments highlight how employing metrics indicative of behavioural activity in early user-game interactions allowed our model to estimate proxy measures of future disengagement and sustained engagement. This suggests that the early user-game interactions might be relevant for characterizing long-term engagement as well as that measures of behavioural activity could be a useful index for its inference \cite{milovsevic2017early, mirza2013does}.
This is in accordance with the aforementioned theoretical formalization of engagement as a dynamic process rather than a static construct \cite{o2008user}. 
This could indicate that recurrency both improves model fit and allows for more efficient use of the available parameters.
This show how our intuition about the importance of past behavioural intensity information for predicting future intisity is confirmed and how having an architecture able to model dynamical system with memory can help leverage all the historical information. 

