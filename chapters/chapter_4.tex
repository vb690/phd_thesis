\section{Introduction}
\label{representation_analysis_introduction}
In this chapter we will proceed at analyzing the representations inferred by the architectures developed in chapter \ref{chapter_implementation_testing}. Our aim will be to probe them in the attempt to evaluate if they possess some of the functional characteristics proper of attributed incentive salience. Indeed, despite the predictive properties assessed in chapter \ref{chapter_implementation_testing} are a necessary condition for a good approximation of the motivational state  of an individual, they are surely not sufficient. Differently from what has been done in chapter \ref{chapter_implementation_testing}, this time we will conduct our investigation using  a combination of dimensionality reduction, unsupervised learning and visual analysis.  We will focus on evaluating differences and similarities between the representations derived from the RNN architecture and its extension (i.e. RNN with environmental and game events covariates) as they represent more stable versions of our methodology. We will first briefly introduce the concept that ANN learn a manifold structure of the data and embed it in a high dimensional latent representation. Subsequently we will illustarted how it is possible to visualize these manifold structures by means of appropriate dimensionality reduction techniques and the procedure we followed for extracting the latent representation produced by our ANN architectures. After that we will define which type of functional characteristics we expect the representation to show and what differences we expect to see between the representations generated by the different models. Finally by means of unspervised learning we will conduct a partition analysis on the representation generated by the ANN in order to individuate profiles able to map the representation generated by the models back to the observable behavioural space of the input data.The steps of the analysis pipeline used for evaluating the generated latent representations are illustrated in figure \ref{fig: pipeline_inspect}.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{images/chapter_4/pipeline_inspect.png}
    \caption[\textbf{Representation analysis experimental pipeline}]{Arrows indicate the flow of the pipeline. Big coloured blocks are major pipeline steps, white rectangles indicate sub-tasks within each step. This experimental pipeline stems directly from the "Model Evaluation" stage outline in figure \ref{fig: pipeline_eval}.}
    \label{fig: pipeline_inspect}
\end{figure}

\section{Extracting and Visualizing the Latent Representation}
\label{extract_visulize}

\subsection{Manifold, Embedding and Neural Networks}
\label{manifold_learning_embed}
As we mentioned in section \ref{chapter_theory_modelling}, when trying to approximate a certain function, ANNs work under the assumption that the input they receive can be effectively represented on a lower dimensional manifold and by moving along this manifold we can reach inputs with different characteristics leading to variations in the predictions produced by the model. That said, the manifold structure learned by the model, despite being potentially low dimensional it is stored, it said it is embedded, again in a higher dimensional space \cite{bengio2017deep} that despite being potentially smaller than the original input it is still challenging to parse for the average human being. Intuitively we can think that the instructions on how to extract the low dimensional manifold from the input data are stored in a distributed way in all the parameters making up that part of the network in charge of generating the latent representation. In our case if we look at Figures \ref{fig: rnn_2} and \ref{fig: rnn_env_even}, the portion of the graphs marked in red should, once the architectures have been fitted on the data,  provide us with a representation that approximately informs us on the motivational state of the individual (see chapter \ref{chapter_theory_modelling} for the theoretical reasons for this assertion). As illustrated in sections \ref{artificial_neural_networks} and \ref{manifold_learning}, since an ANN can be considered a directed acyclic computational graph, to obtain the representation produced in any point of a specific architecture it is sufficient to pass an input through all the operations performed before that point. Figure \ref{fig: repr_extr} illustrate the process for the RNN architecture
\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{images/chapter_4/representation_extractor.png}
    \caption[\textbf{The procedure for generating latent representations generated by an ANN}]{Orange and green shapes represent respectively embedding and LSTM layers. Embedding layers are a type of feedforward layers specifically designed for dealing with categorical inputs \cite{chollet2015keras}. Gray shapes indicate operations with no learnable parameters, such as tensor instantiation and concatenation. The orange transparent shape indicate the concatenation of a single embedding with multiple tensors. Stacked, transparent colouring indicates arrays with a sequential structure. Straight and curved arrows refer to the presence of feed-forward or recurrent information flow. The red highlight shows the portion of the model we hypothesize is inferring an approximation of attributed incentive salience. Given inputs $O \in \mathbb{Z}^{N \times t}$ and $B \in \mathbb{R}^{N \times t \time 5}$, the matrix $Z_t \mathbb{R}^{N \times h}$ represents the $h$ dimensional (where $h$ is the number of hidden units in the recurrent layer) representation generated by the ANN at time $t$ after all operations in the computational graphs have been performed.}
    \label{fig: repr_extr}
\end{figure}
Borrowing the terminology from the unsupervised deep learning literature \cite{bengio2017deep} we call this truncated version of the original architecture an encoder. Encoders can be thought as functions (which parameters have been learned during the fitting procedure) mapping input data onto the manifold space learned by the original architecture.

\subsection{Dimensionality Reduction and Manifold Approximation}
\label{dim_reduction}
If we look at figure \ref{fig: repr_extr} we can see that with increasing size of $h$ it becomes challenging to inspect the representation generated by the encoder. However If we recall from section \ref{manifold_learning} the intrinsic dimensionality for this representation should be much smaller, in case of a latent state like attributed incentive salience this could be as small as one dimensional, two if we consider the nature of the rewarding object (see section \ref{motivation_hist} and figure \ref{fig: vect_mot} in particular). In this view a natural approach for inspecting the shape of the learned manifold would be to perfrom some form of dimensionality reduction and since we expect attributed incentive salience to behave like a two dimensional vector, Principal Component Analysis (PCA) \cite{pearson1901liii} seems to  the most reasonable approach. However the choice of which algorithm to chose is not necessarily that straightforward. Looking at figure \ref{fig: swiss_ambient}, we can see the example of a dataset constituted by three separate (the separation aspect is important and will be further in section \ref{functional_properties}) point clouds (denoted by different colours) in a three dimensional ambient space (the space in which a low dimensional manifold might be embedded). Red and green clouds are examples of the synthetic Swiss Roll dataset \cite{scikit-learn}, while the blue cloud is a simple random projection of a square. Both datasets are basically equivalent (i.e. intrinsically two dimensional with the main dimension defined by the gradient colouring) but differs in their layout in ambient space: the square is linear while the Swiss roll warps in a non linear fashion.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{images/chapter_4/ambient.png}
    \caption[\textbf{Swiss rolls in ambient space}]{Swiss rolls in ambient space}
    \label{fig: swiss_ambient}
\end{figure}
Figure \ref{fig: swiss_reduce} show the performance on the ataset of PCA along with an alternative non-linear dimensionality reduction approach: the Uniform Manifold Approximation and Projection (UMAP) \cite{2018arXivUMAP}. 
\label{dim_reduction}
\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{images/chapter_4/reduced.png}
    \caption[\textbf{PCA and UMAP reduction of Swiss rolls}]{Swiss roles reduced}
    \label{fig: swiss_reduce}
\end{figure}
The UMAP algorithm is a dimensionality reduction technique based on manifold learning. Given a high dimensional dataset, UMAP first infers its topological structure by means of a k-nearest neighbours graph and then, using stochastic gradient descent, attempts to structurally reproduce it in a lower dimensional space (two or three for visualization purposes) \cite{2018arXivUMAP}. Compared to other similar approaches (for example, the t-distributed Stochastic Neighbor Embedding \cite{van2008visualizing}), despite being a local algorithm, UMAP has the ability to better preserve also the global structure of the original data. Moreover, when given datasets that are sequential in natures (like those produce by the recurrent part of our architecture) UMAP is able to include this characteristics during the optimization process \footnote{See \cite{alignedumap} for implementation details.} generating lower dimensional representations that are coherent over time. What we observe here is that both techniques manage to keep the separation between point clouds, PCA, differently from UMAP, struggle to faithfully represent the the intrinsic structure of the data. This toy example is particularly relevant in our case as the representation generated by ANNs are by design highly non-linear even if the manifold structure they inferred might be. This will be made evident in the next section where we will proceed at visualizing the representation learning by our ANNs architecture.

\section{Representation Analysis}
\label{representation_analysis}
In order to support our idea that the representation learned by the different architecture was indeed approximating the construct of incentive salience, we carried out a series of qualitative analyses. If our intuitions were corrected we expected the low dimensional manifold inferred by the architectures to exhibit a sere of characteristics:
\begin{enumerate}
    \item The effective dimensionality in the representation extracted from the network is much smaller than the observed one.
    \item The representation is able to effectively distinguish between different game objects.
    \item The representation is able to effectively distinguish between individuals based on the expected intensity of future interactions
    \item The aforementioned characteristics are consistent over time
\end{enumerate}
The characteristics mentioned above are concerned with a general validation of the properties of the latent representation for all the considered architectures, however we also wanted to understand the contribution of the included covariates in the generation of a more proficient latent representation.  Hence, we preceded at inspecting also the representation generated by the recurrent layers used for modelling the contribution of enviornmental and game event information. The procedure followed for extracting the latent representation was the same for all architectures. First, we re-fitted all models on a random sample (i.e. 90\%) of the validation-set following the same procedures specified in chapter \ref{chapter_implementation_testing}. Then, we created 6 encoders using the approach illustrated in paragraph \ref{manifold_learning_embed} and Figure \ref{fig: repr_extr}. Two were used for extracting the representations expected to approximate the level of attributed incentive salience (red highlights in Figures \ref{fig: rnn_2} \ref{fig: rnn_env_even)}. One for extracting the same type of representation inferred however by the MLP architecture (this was done for comparative purposes). And  three for extracting the intermediate representations generated by the improved version of the RNN architecture and. Subsequently, we passed the remaining portion of the validation-set (i.e. 10\%) as an input to the encoders, producing arrays of shape $(N \times T \times h)$ with $N$ being the number of considered individuals, $h$ the number of hidden units in the last layer of the encoder and $T$ the number of observed interactions for the considered individuals. In our case, since all architectures were of type sequence-to-sequence we were able to analyze not just the representation at specific point in time but also the evolution dynamics. To summarize, we can say that the encoder provided by each ANN architecture was tasked to generate a high dimensional representation where distance could be interpreted as similarity between individuals with respect to the intensity of their future interactions with a specific game object (see the manifold hypothesis of attributed incentive salience presented in paragraph \ref{manifold_rep_incentive_salience}). Dimensionality reduction was then used for approximating the intrinsic manifold structure on a 2 dimensional plane and allowing to qualitatively analyse it through visual inspection. Using the UMAP algorithm, we inferred the topological structure of the produced representations by computing the cosine distance in a local neighborhood of 1000 points with a minimum distance of 0.8. The projection on a two dimensional plane was then achieved by running the optimization part of the algorithm for 2000 iterations. The choice of a large neighborhood and minimum distance was made to better capture the global structure of the representation space \footnote{See \cite{umapwebs} for a visualization of the effects of these hyperparameters in UMAP.}. In order to gather an understanding on the characteristics of the function used for generating the latent representations, we conducted a set of purely exploratory investigations of the relationship between hidden units' activation in the recurrent layers and the predictions produced by the model. To quantify the strength of the observed relationship we employed the Maximal Information Coefficient (MIC) \cite{reshef2011detecting}, a measure of mutual information that can quantify both linear and non-linear association between variables. The MIC can assume values between 0 to 1 with 1 corresponding to a perfect association. We adopted the implementation of UMAP provided McInnes \textit{et. al.} \cite{mcinnes2018umap-software} while the MIC was computed using the python library minepy \cite{albanese2013minerva}. Visualizations were produced using the python libraries matplotlib \cite{hunter2007matplotlib} and seaborn \cite{waskom2021seaborn}.

\subsection{Validating the Functional Properties of the Latent Representation}
\label{functional_properties}
As a first thing we wanted to investigate if the assumption about the low dimensionality of the manifold could hold. Looking at figure \ref{cross_corr_act}
\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{images/chapter_4/corr_matrices.png.png}
\caption[\textbf{Cross-correlation analysis of the hidden units activation of the RNN architecture}]{Each panel shows the cross correlation between the activity of the RNN's artificial neurons in all the game objects going from $t1$ to $t4$. The y and x axes are symmetrical and identifies the RNN artificial neurons while the coloured cells report the Spearman's Rho correlation coefficient for the activation of each pair of neurons. White cells represent combinations for which the correlation coefficient resulted lower than 0.05.}
\label{cross_corr_act} 
\end{figure}
we can observe consistent patterns of cross-correlation for the activity of the hidden units constituting the latent representation, suggesting the presence of redundancy. In order to support this finding and for gathering a general sense of the actual intrinsic dimensionality of the manifold we were trying to approximate, we  conducted a Principal Component Analysis (PCA). Despite PCA and UMAP working under radically different assumptions and mechanisms, we thought this could provides us with a lower bound of how much variance we would be able to capture considering only two dimensions. 
\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{images/chapter_4/pca_embedding.png.png}
\caption[\textbf{Principal component analysis of the hidden units activation of the RNN architecture }]{ \textbf{The panel shows the percentage of explained variance by considering 2 to 20 principal components for each game object going from $t1$ to $t4$. The y axis indicates the percentage of explained variance while the x axis the number of principal components considered.}
\label{pca_emb} 
\end{figure}
Looking at figure \ref{pca_emb} we can see that two principal components can explain a large portion of variance in the representation generated by the RNN. Reducing the inferred representation to only two dimensions, PCA was able to explain from 30 to 60\% of the variance, with maximum explanatory power around 6 and 8 principal components.


Inspecting the representation generated by the RNN model at $t1$ (see Figure \ref{full_panel_static}A) we observe that the model was able to effectively distinguish between different game objects while simultaneously encoding for variations in the expected intensity of future interactions. This is illustrated by the fact that each game object occupies different and distinct regions in the representation space while showing a within-object gradient-like organization that places individuals (i.e. single dots) on a continuum based on the estimated magnitude of their future behaviour. This organization is preserved for each of the six targets showing how the representation inferred by the model is a suitable meta-descriptor for different behavioural indicators. As expected, some targets show a very similar but not identical organization (e.g. Future Session Time and Future Session Activity) while others appear to be independent (e.g. Future Session Time and Future Absence). We note that the absolute location of each game aggregate (i.e. all the points belonging to a specific game object) on the 2D plane is arbitrary. As we can see in figures \ref{full_panel_static} and \ref{full_panel_temporal}, this will change at every run of the algorithm due to the stochastic nature of UMAP. Panels \ref{full_panel_static}B and \ref{full_panel_static}C provide more insight into the activation profiles of individual hidden units constituting the generated representation. Panel \ref{full_panel_static}B shows the relationship between the activity of 10 randomly-chosen units and the predictions generated for the five targets. These are essentially transducer functions illustrating how the estimate for a particular target varies (on average) as the output of a units increases or decreases. Each unit seems to encode for multiple non-monotonic functions, one for each of the considered targets. Differences in the shape of these functions reflect similarities between their associated targets. For example, the functions associated to two highly related targets like Future Session Time and Future Session Activity (see panel \ref{full_panel_static}A) appear to be very similar in shape (see panels \ref{full_panel_static}B and \ref{full_panel_static}C). Interestingly, although most units appear to encode for unique functions some of them (e.g. 41 and 44) show an almost identical behaviour. This suggests the presence of redundancy in the functions underlying the representation generated by the RNN model. These observations are clarified in panel \ref{full_panel_static}C, where the functions associated with a single unit (20, indicated by a dark box in \ref{full_panel_static}B) are presented. Here we observe a strong, non-linear relationship between the unit's activity and the estimated targets (see the high MIC values and the line of best fit). In addition, the between-targets variation in MIC values suggest how the chosen unit is not equally informative for all targets.

\begin{figure}[ht]
\centering
\includegraphics[width=0.7\textwidth]{images/chapter_4/static_repr_42.png}
\caption[\textbf{Lower dimensional representation of the latent state generated by the RNN architecture}]{The representation generated by the RNN model distinguishes between different game objects while maintaining an overarching organization able to capture variations in the expected intensity of future interactions that individuals will have with a specific game object. Panel A shows the two-dimensional projection, produced by UMAP, of the multi-dimensional representation inferred by the RNN at $\mathbf{t1}$ as produced by UMAP. We can read the values of the x and y axes as a coordinate system where proximity represents similarity between points in the original high-dimensional space. Each point indicates the representation inferred by the RNN model after observing one game session from a single user. The colours in the Game Context panel indicate the game object from which the representation is coming. Colours in the small panels represent the discounted sum of all future predictions for a particular target (for example, estimated Future Session Time) $\widehat{B}_{t2:T}$ which is given by $\sum_{i=0}^{t2:T} \gamma^i\widehat{B_i}$ with $\gamma=0.1$ as illustrated in equation \ref{td_v}. \textbf{Each unit  encodes the intensity of future interactions through multiple non-monotonic functions}. Panels B and C show the relationship between the activation of randomly-selected hidden units in the LSTM layer of the RNN and the model's predictions at $\mathbf{t1}$. Panel B shows the relationship between the discretized activation of 10 randomly selected units (artificial neurons) plotted along the y axis and the predictions made by the model at $t1$ (colour coded from blue to red as in the small panels in A) for the game object $hmg$. Panel C shows in more detail the relationship between discretized activation and RNN predictions for a single unit highlighted by a black box in Panel B. Here the x axis indicates the discretized activation while the y axis the mean discretized discounted sum of all future predictions produced by the model. Vertical lines are standard errors of the mean. The red curve is the line of best fit provided by a generalized additive model \cite{serven2018} while the box reports the MIC and the correlation coefficient (Spearman's $\rho$) between the artificial neuron activation and the model's predictions.}
\label{full_panel_static}
\end{figure}
The analyses in Figure \ref{full_panel_static} were performed at a single time point $t1$. When performed at subsequent time points the results appear to be qualitatively similar. For example, focusing on Future Session Time (see Appendix \ref{appendix_representation} for results connected to other targets), we see in Figure \ref{full_panel_temporal}A that the model's ability to segregate different game objects while providing an  overarching representation of the intensity of future interactions is preserved over time. This supports the hypothesis that the representation inferred by our model is dynamic in nature which is further corroborated by panel \ref{full_panel_temporal}D. There we can see how the RNN model was able to individuate a "space" with temporally consistent ”hot” and ”cold” regions between which individuals moved over time depending on the expected intensity of their future interactions. This means that given the history of interaction of a particular individual with a specific game object, our model would determine their "position" (i.e. their "internal state") in the "attributed incentive salience space". This aligns with the manifold hypothesis mentioned in sections \ref{manifold_state} and \ref{manifold_learning}: changes in the propensity to interact with a specific game object (i.e. variations in the amount of attributed incentive salience) can be expressed moving on a manifold embedded within an $h$ dimensional space, with $h$ being the dimensionality of the representation generated by our RNN model. It appears that the hidden units constituting this representation tend to be consistent over time in the type of functions they encode (see Figure \ref{full_panel_temporal}B and C). As expected, we can again observe a strong non linear association between units' activation and targets' predictions, see MIC values and lines of best fit. The decrease in MIC value observed in Figure \ref{full_panel_temporal}C for the artificial neuron 72 might indicate how certain units lose their informative power over time.
\begin{figure}[ht]
\centering
\includegraphics[width=0.7\textwidth]{images/chapter_4/dynamic_repr_42.png}
\caption[\textbf{Lower dimensional representation of the evolution of the latent states generated by the RNN architecture}]{The representation generated by the RNN model appears to maintain its discriminant properties over time. Panel A shows a two-dimensional projection of the multi-dimensional representation inferred by the RNN at $t2$, $t3$ and $t4$. The inferred representation maintains its gradient-like organization over time with an increased ability to differentiate between game objects. As in Figure \ref{full_panel_static}, x and y axes are dimensions individuated by the UMAP algorithm and can be interpreted as a coordinate system where proximity represents similarity between points. Colours in the first row indicate which game object the representation is coming from while those in the second row indicate the discounted sum of future predictions for a single target (i.e. "Future Session Time"). \textbf{The units constituting the generated representation encode for functions that are consistent over time.} Panels B and C show the relationship between units' activation and the model's predictions over time for the game object $hmg$. Different units appear to encode the same target with different non non-monotonic functions which are relatively consistent over time. Panel B illustrates the relationship between the same 10 randomly selected units specified in figure \ref{full_panel_static} and the predictions made by the model for Future Session Time at $t2$, $t3$ and $t4$. Panel C shows in more detail the relationship of the three artificial neurons, highlighted by black boxes in B, across time. Each row is a different unit while each column corresponds to a different $t$. The x axis indicates the discretized activation while the y axis the mean discretized discounted sum of all future predictions. Vertical lines are standard errors of the mean. The red curve is the line of best fit provided by a generalized additive model \cite{serven2018} while the box report the MIC and the correlation coefficient (Spearman's $\rho$) between the artificial neuron activation and the model's predictions. \textbf{The generated representation produces areas of low and high expected intensity among which individuals move over time.} Panel D shows trajectories through time produced by a version of UMAP that incorporates temporal information. Data are drawn from random subsets of individuals having low, medium and high variability in their expected amount of future behaviour. The representation inferred by the RNN model produces "hot" (i.e. the left side) and "cold" (i.e. the right side) regions, representing high and low expected Future Session Time, that are spatially consistent over time. Individuals appear to either stay in the same region or to move between regions over time. Here each line represents variations in the representation generated by the RNN model for a single user over four temporal steps. Continuity is generated by means of cubic spline interpolation for the lines and by linear interpolation for the colours. The x and y axes are the dimensions individuated by the UMAP algorithm while the z axis indicates the associated point in time. Colours indicate the discounted sum of future predictions produced by the model at a specific point in time.}
\label{full_panel_temporal}
\end{figure}

As we mentioned in section \ref{comp_framework}, both ANNs try to predict the intensity of future behaviour given the history of interactions. They do so relying on the same type of metrics, leveraging similar computational mechanisms (i.e. multitask learning and non-linearity) and producing representation according to the same underlying principle (i.e. the manifold hypothesis). Nevertheless, the fact that MLP provides poorer fit to data already suggests that whatever representation it has inferred it is likely a sub-optimal approximation of the manifold structure of incentive salience. Looking at figure \ref{predictive_panel}A, and knowing that UMAP represents differences and similarities between points through distance, we can see how the representation generated by the MLP less clearly differentiate between game objects. On the same figure, we can notice how the gradient representation for the metric Future N° Sessions Time is largely disrupted. This effect is however consistently less pronounced for other metrics (see our \href{https://htmlpreview.github.io/?https://github.com/vb690/approx_incentive_salience/blob/main/notebooks_html/embedding_analysis.html}{GitHub} for additional visualizations), in accordance to the differences we observed in predictive performance (see Figure \ref{model_comp_non_coll}). Recalling what mentioned in section \ref{comp_framework}, the latent state produced by the level of attributed incentive salience should retain at any point in time some predictive power over the intensity of all the future interactions (i.e. not just the one that follows). Figure \ref{rnn_future}B shows the representation generated by RNN and MLP at $t1$ but color coded with the discounted sum of the predictions made from $t4$ onward. We can see that, even if degraded, RNN still preserves some of the desired gradient-like organization which is instead much more disrupted for MLP. This is in accordance to what is shown by Figure \ref{full_panel_temporal}D: the RNN appears to define regions of high and low expected behavioural intensity which are consistent over time rather than constrained to the region around $t+1$.
\begin{figure}[ht]
\centering
\includegraphics[width=0.7\textwidth]{images/chapter_4/RNN_MLP_repr_42.png}
\caption[\textbf{Lower dimensional representation of the latent states generated by the time-distributed MLP architecture}]{The representation generated by the MLP model is less effective at distinguishing between different game objects and different levels of expected future behaviour intensity. Panel A shows a two-dimensional projection of the multi-dimensional representation inferred by the MLP at $t1$, $t2$, $t3$ and $t4$. Differently from the RNN, the representation shows a disruption in the gradient-like organization and a reduced ability to differentiate between game objects which remain constant over time. The x and y axes are dimensions individuated by the UMAP algorithm and can be interpreted as a coordinate system where proximity represents similarity between points. Colours in the first row indicate which game object the representation is coming from while those in the second row indicate the discounted sum of future predictions for a single target (i.e. "Future N° of Sessions") \textbf{The representation generated by the MLP model is less effective at at distinguishing different levels of expected behaviour intensity for states that are further away in the future.} Panel B shows a two-dimensional projection of the multi-dimensional representation inferred by the RNN (left) and MLP(right) at $t1$ but colour coded with the discounted sum of future predictions from $t4$ onward. The representation generated by the RNN is able to maintain a gradient-like organization even from states that are further away in the future while this capacity is almost entirely lost for the MLP. The colours in the Game Context panel indicate the game object from which the representation is coming. Colours in the small panels represent the discounted sum of all future predictions for a particular target computed from $t4$ onward instead that from $t1$. The x and y axes are the dimensions individuated by the UMAP algorithm.}
\label{predictive_panel}
\end{figure}

\subsection{Evaluating the Contribution of environmental and game events covariates in improving the latent representation.}
\lorem

\section{Partition Analysis}
\label{partition_analysese}
We conducted a partition analysis to individuate behavioural profiles associated with the representation generated by our model. As specified in section \ref{manifold_learning} the representation extracted by the encoder at time $t$ can be interpreted as a set of coordinates on the manifold generated by the RNN model after observing $t$ game sessions. Partitioning this representation allows us to identify areas of the manifold that hold information about the history of interactions between an individual and a video game object. These areas may represent variations in the levels of attributed incentive salience and therefore be associated with distinct patterns of behaviour. To partition the data, we used an unsupervised approach applying Mini-Batch K-Means \cite{sculley2010web}, a variation of K-Means, to the representation extracted by the encoder. Given a dataset, the algorithm attempts to divide it by iteratively moving $k$ centroids so as to reduce variance within each partition. The choice of Mini-Batch K-Means was dictated by the fact that it is one of the few distance-based algorithms that scales to very large datasets. To select the optimal $k$ value, we first fitted the algorithm with a varying number of centroids (i.e. 2 to 10) and computed the associated "inertia" (here, a measure of within cluster variance). Since inertia tends to zero as $k$ approaches the number of points in the dataset, we defined the optimal number of partitions as the value of $k$ at which the inertia reached its "elbow" or maximum curvature \cite{satopaa2011finding}. This allows to individuate at which number of partitions there are diminishing returns in terms of within cluster variance reduction. Every instance of Mini-Batch K-Means was initialized 3000 times at random and ran for a maximum of 3000 epochs. The input data were re-scaled to have zero mean and unit-variance and passed to the algorithm in random batches of size $(512 \times h)$. The associated behavioural profiles were found by applying this methodology separately to each game object and retrieving for each partition the mean of all the behavioural metrics over time. The Mini-Batch K-Means implementation used for this analysis was provided by the python library scikit-learn \cite{scikit-learn}. \\
\\
All the analyses were conducted using Python programming language version 3.6.2 \cite{10.5555/1593511}.

\subsection{Partitioning the representation associated to the covariates}
\lorem

\subsection{Partitioning the behavioural representation}
\lorem

\subsection{Discussion}
As mentioned in section  \ref{incentive_salience}, incentive salience attribution produces latent representations of objects which, when imbued with value, make future interactions with those objects more likely and intense \cite{berridge1998role,berridge2004motivation}. The representation generated by our model showed similar functional properties in their global-local organization. At the global level, different game-objects were organized in distinct and coherent regions (see Figure \ref{full_panel_static}A) showing how the model attempted to operate on a meta-level by partitioning a global representation in several object-specific ones. This finding aligns with what highlighted in various work on neural manifold where the responses related to qualitatively different stimuli tends to show a cluster-like organization when reduced to a lower dimensional space \cite{stopfer2003intensity, gallego2017neural, ganmor2015thesaurus}. At the local level, each object-specific representation showed an internal gradient-like organization distinguishing individuals based on the estimated intensity of their future interactions with that specific object. This was true for each of the considered behavioural targets (see Figure \ref{full_panel_static}A) showing how the model attempted to provide an holistic description of the intensity of future interactions. The presence of this type of gradient-like organization emerged in a work by Nieh et al. \cite{nieh2021geometry} when analyzing neural responses during an evidence accumulation task in virtual reality. When reducing the neural activity to a 3 dimensional space, the resulting manifold presented a clear gradient able to code simultaneously for position and levels of accumulated evidence \cite{nieh2021geometry}. A similar finding was present in the work by Stopfner et al. \cite{stopfer2003intensity} where the manifold structure extracted from the activity of olfactory neurons was able to represent qualitative and quantitative differences between odours through a global-local organization similar to that showed in section \ref{repr_results}. The dynamic nature of the representation generated by our approach also nicely fits with that of attributed incentive salience \cite{toates1994comparing,robinson1993neural,zhang2009neural,tindell2009dynamic,berridge2012prediction}. In particular, the fact that the aforementioned global-local organization is maintained over time (see Figure \ref{full_panel_temporal}A) corroborate the hypothesis that our model approximated state changes originated from a dynamic process. In support of this, we also observed that the representation generated by our model was spatially coherent over time: it produced distinct regions of low and high expected intensity between which individuals moved over time (see Figure\ref{full_panel_temporal}D). These results appear to match the definition of motivation and incentive salience attribution specified in section \ref{motivation}: a single overarching process able to dynamically predict the likelyhood and intensity by which individuals will interact with a varied set of objects \cite{simpson2016behavioral,toates1994comparing,berridge2004motivation,zhang2009neural}. Many other cognitive and affective functions might rely on a latent representation that is functionally similar to the one described in our work (e.g. credit assignment and optimal control \cite{wang2018prefrontal, barto2004reinforcement}, cognitive control, learning \cite{skinner1965science} or various forms of reward processing \cite{schultz1997neural, schultz2000reward}). Similarly to attributed incentive salience, these functions are all involved in generating motivated behaviour and heavily rely on reward signals, however none of them is concerned with attributing and describing the motivational saliency that an object possess. This is made evident in the works by McClure et al. \cite{mcclure2003computational} and Zhang et al. \cite{zhang2009neural} where the system involved in salience attribution is functionally separate from the one assigning credit and executing actions: the former provide a representation that informs and biases the decisions taken by the latter serving an almost exclusively qualifying role (see the role of attributed incentive salience in addiction-like conditions \cite{robinson1993neural}). Similarly, the representation generated by our model doesn't provide any insight on the decision making process underlying the observed playing behaviour but simply provide an approximate description of the "motivational pull" that a particular game object has on a particular individual at a certain point in time. The functions encoded by the hidden units constituting the representation appeared to have a series of distinctive properties, namely: redundancy, non linearity, multiplicity (single units code for multiple functions) and consistency over time. These may have played a role in providing the representation generated by our model with its distinctive characteristics. For example, as we mentioned in section \ref{manifold_state} redundancy and inter-correlation are characteristics of the signals from which the manifold representation of internal states arises \cite{seung2000manifold,gallego2017neural}. Multiplicity on the other hand, might be the factor underlying the ability of our model to produce a single unitary representation which holds predictive power over different behavioural targets. Finally, consistency over time could be the mechanisms supporting the type of temporal coherence observed in panel \ref{full_panel_temporal}D. We want to stress that these findings are to be considered exploratory in nature since they do not rely on a-priori hypotheses. A comparison between these computational properties and those underlying the attribution of incentive salience is required and would constitute a potential venue for future investigations. This supports the idea that our approach, by giving full access to its constituent parts, provides a certain degree of interpretability and offers the possibility of generating testable hypotheses. The partition analysis revealed a set of diverse profiles that largely reflect expected behavioural correlates of different levels of attributed incentive salience (i.e. high vs low intensity profiles) \cite{berridge2004motivation}. The various offsets that each partition showed might suggest different levels of predisposition towards the individual game-objects. The dynamic nature of these profiles provided a more granular characterization allowing to observe variations in the entire history of interactions and not just in the expected intensity of future ones. For example, it was possible to see how a higher likelihood of future interactions was supported both by a history of low intensity but high frequency interactions as well as by a series of high frequency and high intensity interactions (see partitions 1 and 2 in Figure\ref{partitioning}B). In this sense, these behavioural profiles can be seen as useful devices for investigating the existence of inter-individual differences in schedules of interactions with potentially rewarding objects.
