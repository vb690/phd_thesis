\chapter{Frequent Notation}

\section{Identity function}
Given a vector $x$ the identity function $id$ is defined as 
\begin{gather}
    \label{identity}
    id(x) = x
\end{gather}

\section{Sigmoid function}
Given a vector $x$ the sigmoid function $\sigma$ is defined as 
\begin{gather}
    \label{sigmoid}
    \sigma(x) = \frac {1} {1 + e^{-x}}
\end{gather}

\section{Hyperbolic Function}
Given a vector $x$ the hyperbolic function $\tanh$ is defined as 
\begin{gather}
    \label{tanh}
    \tanh(x) = \frac {e^{2x} -1} {e^{2x} +1}
\end{gather}

\section{ReLU Function}
Given a vector $x$ the Rectified Linear Unit function $ReLu$ is defined as 
\begin{gather}
    \label{relu}
    ReLU(x) = \max(0, x)
\end{gather}

\section{Leaky ReLU Function}
Given a vector $x$ the Leaky Rectified Linear Unit function $ReLu$ is defined as 
\begin{gather}
    \label{lelu}
    LReLU(x) =  = 
    \begin{cases}
        x,& \text{if } x > 0 \\
        \alpha x,              & \text{otherwise}
    \end{cases}
\end{gather}
with $\alpha=0.3$ being a small constant controlling the slope coefficient for the negative part of the function.

\section{ELU Function}
Given a vector $x$ the Exponential Linear Unit function $ELu$ is defined as 
\begin{gather}
    \label{elu}
    ELU(x) = 
    \begin{cases}
        x,& \text{if } x > 0 \\
        \alpha (e^{x} -1),              & \text{otherwise}
    \end{cases}
\end{gather}
with $\alpha=1$ being the scale for the negative part of the function.

\section{Mean Squared Error}
Given a vector of ground truth values $y \in \mathbb{R^N}$ and predictions $\widehat{y} \in \mathbb{R^N}$ the mean squared error (MSE) is defined as 
\begin{gather}
\label{mse}
    \text{MSE}=
        \dfrac
            {1}
            {N}
            \sum\limits_{i=1}^{N}  (y_i - \hat{y_i})^2
\end{gather}

\section{Symmetric Mean Absolute Percentage Error}
Given a vector of ground truth values $y \in \mathbb{R^N}$ and predictions $\widehat{y} \in \mathbb{R^N}$ the symmetric mean absolute percentage error (SMAPE) is defined as 
\begin{equation}
  \begin{gathered} 
  \label{smape}
     SMAPE(y, \widehat{y}) = 100 *
    \frac{1}{N} 
    \sum_{i=1}^{N}
    \frac{| y_{i} - \widehat{y}_{i} |} {|y_{i}| + |\widehat{y_{i}}|}  
  \end{gathered}
\end{equation}

\section{Binary Cross-Entropy}
Given a vector of ground truth values $y \in \mathbb{Z}_2^N$ and predictions $\widehat{y} \in \mathbb{R}^N_{[0, 1]}$ the Binary Cross-Entropy (BCE)  is defined as 
\begin{gather}
\label{bce}
    \text{BCE}=
        -\dfrac
            {1}
            {N}
        \sum\limits_{i=1}^{N}  y_i \cdot log(\hat{y_i}) + (1-y_i) \cdot log(1 - \hat{y_i})
\end{gather}

\section{F1 Score}
Given a vector of ground truth values $y \in \mathbb{R}^N_{[0, 1]}$ and predictions $\widehat{y} \in \mathbb{R}^N_{[0, 1]}$ the F1 score is defined as 
\begin{gather}
\label{F1}
    \text{F1}=
        2 \cdot 
        \dfrac
            {(precision \cdot recall)}
            {(precision + recall)}
\end{gather}
with $precision =\frac {TP}{(TP + FP)}$ and $recall = \frac {TP}{(TP + FN)}$, where \textit{TP, FP, TN, FN} stand for True Positives, False Positives, True Negatives and False Negatives. 

\section{Dropout Regularization}
Given parameters $\theta \in \mathbb{R}^z$ the dropout regularization is defined as 
\begin{gather}
    \label{dropout}
    dropout(\theta) = \prod^z _{i=1} \theta_i \times d_i \\ \nonumber
    d_i \sim Bernoulli(p) \\ \nonumber
\end{gather}
with $p$ being the probability of a parameter being dropped. 

\section{One Dimensional Spatial Dropout Regularization}
Given a sequence of parameters $\theta_{t_1:T} \in \mathbb{R}^{T \times z}$ the one dimensional spatial dropout regularization is defined as 
\begin{gather}
    \label{spatial_dropout}
    dropout(\theta) = \prod^T _{t=1} \theta_{t:} \times d_t \\ \nonumber
    d_t \sim Bernoulli(p) \\ \nonumber
\end{gather}
with $p$ being the probability of an entire step in the sequence of parameters being dropped. 

\section{Batch Normalization Regularization}
Given an embedding  $h \in \mathbb{R}^{N\times z}$ the batch normalization regularization is defined as 
\begin{gather}
    \label{batch_norm}
    BatchNorm(h) = \frac{h - \mu_b}{\sqrt{(\sigma_b)^2} + \epsilon}
\end{gather}
with $\mu_b$ and $\sigma_b$ being respectively the column-wise mean and standard deviation of the embedding $h$ and $\epsilon$ a small constant for avoiding division by zero.

\section{Ridge Regularization}
Given parameters $\theta$ the ridge regularization function $l2$ is defined as 
\begin{gather}
    \label{ridge}
    l2(\theta) = \lambda \sum_{n=1}^{N}\theta_n^2
\end{gather}
with $\lambda$ being a a constant controlling the strength of regularization.

\section{ElasticNet Regularization}
Given parameters $\theta$ the ridge regularization function $ElasticNet$ is defined as 
\begin{gather}
    \label{enet_reg}
    ElasticNet(\theta) = \lambda (\frac{1 - \alpha}{2}l2(\theta) + \alpha l1(\theta))
\end{gather}
with $\lambda$ and $\alpha$ being constants controlling respectively the global strength of regularization and the contribution of the $l1$ and $l2$ terms to the total ammount of regularization.

\section{Lasso Regularization}
Given parameters $\theta$ the ridge regularization function $l1$ is defined as 
\begin{gather}
    \label{lasso}
    l1(\theta) = \lambda \sum_{n=1}^{N}|\theta_n|
\end{gather}
with $\lambda$ being a a constant controlling the ammount of regularization.

\section{Fully Connected Operation}
Given an input matrix $X \in \mathbb{R}^{N \times h}$, the fully connected operation carried out by $L$ layers feedforward neural network can be defined as
\begin{gather}
    \label{fnn_operation}
    h_0 = X
    h_1 = \phi(\theta_1^\top h_0 + \beta_1)\\ \nonumber
    h_2 = \phi(\theta_2^\top h_1  + \beta_2)\\ \nonumber
    \dots\\ \nonumber
    h_L = \phi(\theta_L^\top h_{L-1}  + \beta_L) \nonumber
\end{gather}
with $\phi$ being a non-linear function, $\{\theta_1, \dots, \theta_N\}$ a set of learnable weights matrices of shape $\theta_l \in \mathbb{R}^{h_{l-1} \times h_{l}}$ and $\{\beta_1, \dots, \beta_N\}$ a set of learnable biases vectors of shape $b_l \in \mathbb{R}^{h_l}$.

\section{One-Hot Encode Operation}
Given an input set of numerical indices $X = \{1, 2, \dots, N\}$, the one-hot encode operation is defined as 
\begin{gather}
    \label{one_hot_encode_operation}
    1_X(x_i) = 
    \begin{cases}
        1,& \text{if } x_i \in X \\
        0,              & \text{otherwise}
    \end{cases}
\end{gather}

\section{Embedding Operation}
Given an input set of numerical indices $X = \{1, 2, \dots, N\}$, the embedding operation is defined as 
\begin{gather}
    \label{embedding_operation}
    h = \phi(\theta_{X,*})
\end{gather}
with $\phi$ being a non-linear function and $\theta$ an $\mathbb{R}^{N \times z}$ learnable weights matrix.

\section{LSTM Cell Operation}
Given an input set of multivariate time series $X_{t_1:T} \in \mathbb{R}^{N \times T \times B}$ with $N$ being the number of series, $T$ the length of the series and $B$ the dimensionality of the series. An LSTM cell operation, applied recursively along the temporal dimension for each $x_{*,t,*} \in X$, is defined as
\begin{gather}
    \label{lstm_operation}
    f_t = \sigma(\theta_{xf}^\top x_{*,t,*} + \theta_{hf}^\top h_{t-1} + \beta_f) \\ \nonumber
    i_t = \sigma(\theta_{xi}^\top x_{*,t,*} + \theta_{hi}^\top h_{t-1} + \beta_i) \\ \nonumber
    o_t = \tanh(\theta_{xo}^\top x_{*,t,*} + \theta_{ho}^\top h_{t-1} + \beta_o) \\ \nonumber
    \widehat{c}_t = \sigma(\theta_{xc}^\top x_{*,t,*} + \theta_{hc}^\top h_{t-1} + \beta_c) \\ \nonumber
    c_t = f_t \times c_{t-1} + i_t \times \widehat{c}_t \\ \nonumber
    h_t = o_t \times \tanh(c_t) \\ \nonumber
\end{gather}
with $\sigma$  being the sigmoid function, $\tanh$ the hyperbolic function, $\theta_{xf}$, $\theta_{hf}$, $\theta_{xi}$, $\theta_{hi}$, $\theta_{xo}$, $\theta_{ho}$, $\theta_{xc}$, $\theta_{hc}$ a set of learnable weights matrices, $\beta_f$, $\beta_i$, $\beta_o$, $\beta_c$, a set of learnable biases, $c_t$ the value at time $t$ of the conveyor belt matrix and $h_t$ the value at time $t$ of the hidden state matrix.


\section{Time Distributed Operation}
Given an input tensor $X \in \mathbb{R}^{N \times T \times h}$, a function $f$ with parameters $\theta$ can be applied along the temporal dimension for each $x_{*,t,*} \in X$ as
\begin{gather}
    \label{time_distributed}
    h_t = f(x_{*,t,*}; \theta)
\end{gather}
it has to be noted that this has the effect of sharing $\theta$ across all $t \in T$ but without temporal conditioning (e.g. like the LSTM operation presented in Appendix \ref{lstm_operation} does).

\section{One Dimensional Convolution Operation}
Given an input set of multivariate time series $X_{t_1:T} \in \mathbb{R}^{N \times T \times B}$ with $N$ being the number of series, $T$ the length of the series and $B$ the dimensionality of the series. A One Dimensional Convolution operation, applied recursively along the temporal dimension for each $x_{*,t,*} \in X$, is defined as
\begin{gather}
    \label{1d_conv}
    h_t = \phi(\theta_{K}^\top x_{*,t : t+k,*})
\end{gather}
with $\phi$ being a non-linear function and $K$ a $K \in \mathbb{R}^{k \times z}$ learnable weights matrix.

\section{One Dimensional Global Average Pooling Operation}
Given an input set of multivariate time series $X_{t_1:T} \in \mathbb{R}^{N \times T \times B}$ with $N$ being the number of series, $T$ the length of the series and $B$ the dimensionality of the series. A Global Average Pooling Operation, is defined as
\begin{gather}
    \label{1d_pool}
    h_t = \dfrac{1}{T} \sum_{t=1}^{t}X_{*,t,*}
\end{gather}

