\section{Identity function}
Given a vector $x$ the identity function $id$ is defined as 
\begin{gather}
    \label{identity}
    id(x) = x
\end{gather}

\section{Sigmoid function}
Given a vector $x$ the sigmoid function $\sigma$ is defined as 
\begin{gather}
    \label{sigmoid}
    \sigma(x) = \frac {1} {1 + e^{-x}}
\end{gather}

\section{Hyperbolic Function}
Given a vector $x$ the hyperbolic function $\tanh$ is defined as 
\begin{gather}
    \label{tanh}
    \tanh(x) = \frac {e^{2x} -1} {e^{2x} +1}
\end{gather}

\section{ReLU Function}
Given a vector $x$ the Rectified Linear Unit function $ReLu$ is defined as 
\begin{gather}
    \label{relu}
    ReLU(x) = \max(0, x)
\end{gather}

\section{Mean Squared Error Function}
Given a vector of ground truth values $y \in \mathbb{R^N}$ and predictions $\widehat{y} \in \mathbb{R^N}$ the mean squared error (MSE) function is defined as 
\begin{gather}
\label{mse}
    \text{MSE}=
        \dfrac
            {1}
            {N}
            \sum\limits_{i=1}^{N}  (y_i - \hat{y_i})^2
\end{gather}

\section{Symmetric Mean Absolute Percentage Error Function}
Given a vector of ground truth values $y \in \mathbb{R^N}$ and predictions $\widehat{y} \in \mathbb{R^N}$ the symmetric mean absolute percentage error (SMAPE) function is defined as 
\begin{equation}
  \begin{gathered} 
  \label{smape}
     SMAPE(y, \widehat{y}) = 
    \frac{1}{N} 
    \sum_{i=1}^{N}
    \frac{| y_{i} - \widehat{y}_{i} |} {|y_{i}| + |\widehat{y_{i}}|}  
  \end{gathered}
\end{equation}

\section{Binary Cross-Entropy Function}
Given a vector of ground truth values $y \in \mathbb{Z}_2^N$ and predictions $\widehat{y} \in \mathbb{R}^N_{[0, 1]}$ the Binary Cross-Entropy (BCE) function is defined as 
\begin{gather}
\label{bce}
    \text{BCE}=
        -\dfrac
            {1}
            {N}
        \sum\limits_{i=1}^{N}  y_i \cdot log(\hat{y_i}) + (1-y_i) \cdot log(1 - \hat{y_i})
\end{gather}

\section{Dropout Regularization Function}
Given parameters $\theta$ the dropout regularization function is defined as 
\begin{gather}
    \label{dropout}
    dropout(\theta) = \prod^J _{i=1} \theta_i \times z_i \\ \nonumber
    z_i \sim Bernoulli(p) \\ \nonumber
\end{gather}
with $p$ being the probability of a parameter being dropped. 

\section{Batch Normalization Regularization Function}
Given parameters $\theta$ the batch normalization regularization function is defined as 
\begin{gather}
    \label{batch_norm}
    dropout(\theta) = \prod^J _{i=1} \theta_i \times z_i \\ \nonumber
    z_i \sim Bernoulli(p) \\ \nonumber
\end{gather}
with $p$ being the probability of a parameter being dropped. 

\section{Ridge Regularization Function}
Given parameters $\theta$ the ridge regularization function $l2$ is defined as 
\begin{gather}
    \label{ridge}
    l2(\theta) = \lambda \sum_{n=1}^{N}\theta_n^2
\end{gather}
with $\lambda$ being a a constant controlling the strength of regularization.

\section{ElasticNet Regularization Function}
Given parameters $\theta$ the ridge regularization function $ElasticNet$ is defined as 
\begin{gather}
    \label{enet_reg}
    ElasticNet(\theta) = \lambda (\frac{1 - \alpha}{2}l2(\theta) + \alpha l1(\theta))
\end{gather}
with $\lambda$ and $\alpha$ being constants controlling respectively the global strength of regularization and the contribution of the $l1$ and $l2$ terms to the total ammount of regularization.

\section{Lasso Regularization Function}
Given parameters $\theta$ the ridge regularization function $l1$ is defined as 
\begin{gather}
    \label{lasso}
    l1(\theta) = \lambda \sum_{n=1}^{N}|\theta_n|
\end{gather}
with $\lambda$ being a a constant controlling the ammount of regularization.

\section{Fully Connected Operation}
Given an input matrix $X \in \mathbb{R}^{N \times h}$, the fully connected operation carried out by $L$ layers feedforward neural network can be defined as
\begin{gather}
    \label{fnn_operation}
    h_0 = X
    h_1 = \phi(\theta_1^\top h_0 + \beta_1)\\ \nonumber
    h_2 = \phi(\theta_2^\top h_1  + \beta_2)\\ \nonumber
    \dots\\ \nonumber
    h_L = \phi(\theta_L^\top h_{L-1}  + \beta_L) \nonumber
\end{gather}
with $\phi$ being a non-linear function, $\{\theta_1, \dots, \theta_N\}$ a set of learnable weights matrices of shape $\theta_l \in \mathbb{R}^{h_{l-1} \times h_{l}}$ and $\{\beta_1, \dots, \beta_N\}$ a set of learnable biases vectors of shape $b_l \in \mathbb{R}^{h_l}$.

\section{One-Hot Encode Operation}
Given an input set of numerical indices $X = \{1, 2, \dots, N\}$, the one-hot encode operation is defined as 
\begin{gather}
    \label{one_hot_encode_operation}
    1_X(x_i) = 
    \begin{cases}
        1,& \text{if } x_i \in X \\
        0,              & \text{otherwise}
    \end{cases}
\end{gather}

\section{Embedding Operation}
Given an input set of numerical indices $X = \{1, 2, \dots, N\}$, the embedding operation is defined as 
\begin{gather}
    \label{embedding_operation}
    h = \phi(\theta_{X,*})
\end{gather}
with $\phi$ being a non-linear function and $\theta$ an $\mathbb{R}^{N \times z}$ learnable weights matrix.

\section{LSTM Cell Operation}
Given an input series $X_{t_1:T}$ an LSTM cell operation, applied recursively for each $t \in T$, is defined as
\begin{gather}
    \label{lstm_operation}
    f_t = \sigma(\theta_{xf}^\top X_t + \theta_{hf}^\top h_{t-1} + \beta_f) \\ \nonumber
    i_t = \sigma(\theta_{xi}^\top X_t + \theta_{hi}^\top h_{t-1} + \beta_i) \\ \nonumber
    o_t = \tanh(\theta_{xo}^\top X_t + \theta_{ho}^\top h_{t-1} + \beta_o) \\ \nonumber
    \widehat{c}_t = \sigma(\theta_{xc}^\top X_t + \theta_{hc}^\top h_{t-1} + \beta_c) \\ \nonumber
    c_t = f_t \times c_{t-1} + i_t \times \widehat{c}_t \\ \nonumber
    h_t = o_t \times \tanh(c_t) \\ \nonumber
\end{gather}
with $\sigma$  being the sigmoid function, $\tanh$ the hyperbolic function, $\theta_{xf}$, $\theta_{hf}$, $\theta_{xi}$, $\theta_{hi}$, $\theta_{xo}$, $\theta_{ho}$, $\theta_{xc}$, $\theta_{hc}$ a set of learnable weights matrices, $\beta_f$, $\beta_i$, $\beta_o$, $\beta_c$, a set of learnable biases, $c_t$ the value at time $t$ of the conveyor belt matrix and $h_t$ the value at time $t$ of the hidden state matrix.

