\chapter{Frequent Notation}

We will provide here the notation associated with operations frequently used throughout the thesis. Notation is referenced when it appears in the manuscript main body for the first time. Vectors are written in low-case ($x$) while matrices in upper-case ($X$). Parameters are indicated by the Greek letter $\theta$ except for weights ($W$) and biases ($b$) used when defining ANN operations.

\section{Identity function}
Given a vector $x$ the identity function $id$ is defined as 
\begin{gather}
    \label{identity}
    id(x) = x
\end{gather}

\section{Sigmoid function}
Given a vector $x$ the sigmoid function $sigmoid$ is defined as 
\begin{gather}
    \label{sigmoid}
    sigmoid(x) = \frac {1} {1 + e^{-x}}
\end{gather}

\section{Hyperbolic Function}
Given a vector $x$ the hyperbolic function $\tanh$ is defined as 
\begin{gather}
    \label{tanh}
    \tanh(x) = \frac {e^{2x} -1} {e^{2x} +1}
\end{gather}

\section{ReLU Function}
Given a vector $x$ the Rectified Linear Unit function $ReLu$ is defined as 
\begin{gather}
    \label{relu}
    ReLU(x) = \max(0, x)
\end{gather}

\section{Leaky ReLU Function}
Given a vector $x$ the Leaky Rectified Linear Unit function $ReLu$ is defined as 
\begin{gather}
    \label{lelu}
    LReLU(x) =
    \begin{cases}
        x,& \text{if } x > 0 \\
        \alpha x,              & \text{otherwise}
    \end{cases}
\end{gather}
with $\alpha=0.3$ being a small constant controlling the slope coefficient for the negative part of the function.

\section{ELU Function}
Given a vector $x$ the Exponential Linear Unit function $ELu$ is defined as 
\begin{gather}
    \label{elu}
    ELU(x) = 
    \begin{cases}
        x,& \text{if } x > 0 \\
        \alpha (e^{x} -1),              & \text{otherwise}
    \end{cases}
\end{gather}
with $\alpha=1$ being the scale for the negative part of the function.

\section{Mean Squared Error}
Given a vector of ground truth values $y \in \mathbb{R}^N$ and predictions $\widehat{y} \in \mathbb{R}^N$ the mean squared error (MSE) is defined as 
\begin{gather}
\label{mse}
    \text{MSE}=
        \dfrac
            {1}
            {N}
            \sum\limits_{i=1}^{N}  (y_i - \hat{y_i})^2
\end{gather}

\section{Symmetric Mean Absolute Percentage Error}
Given a vector of ground truth values $y \in \mathbb{R}^N$ and predictions $\widehat{y} \in \mathbb{R}^N$ the symmetric mean absolute percentage error (SMAPE) is defined as 
\begin{equation}
  \begin{gathered} 
  \label{smape}
     SMAPE(y, \widehat{y}) = 100 *
    \frac{1}{N} 
    \sum_{i=1}^{N}
    \frac{| y_{i} - \widehat{y}_{i} |} {|y_{i}| + |\widehat{y_{i}}|}  
  \end{gathered}
\end{equation}

\section{Binary Cross-Entropy}
Given a vector of ground truth values $y \in \mathbb{Z}_2^N$ and predictions $\widehat{y} \in \mathbb{R}^N_{[0, 1]}$ the Binary Cross-Entropy (BCE)  is defined as 
\begin{gather}
\label{bce}
    \text{BCE}=
        -\dfrac
            {1}
            {N}
        \sum\limits_{i=1}^{N}  y_i \cdot log(\hat{y_i}) + (1-y_i) \cdot log(1 - \hat{y_i})
\end{gather}

\section{F1 Score}
Given a vector of ground truth values $y \in \mathbb{R}^N_{[0, 1]}$ and predictions $\widehat{y} \in \mathbb{R}^N_{[0, 1]}$ the F1 score is defined as 
\begin{gather}
\label{F1}
    \text{F1}=
        2 \cdot 
        \dfrac
            {(precision \cdot recall)}
            {(precision + recall)}
\end{gather}
with $precision =\frac {TP}{(TP + FP)}$ and $recall = \frac {TP}{(TP + FN)}$, where \textit{TP, FP, TN, FN} stand for True Positives, False Positives, True Negatives and False Negatives. 

\section{Inertia}
Given a matrix $X \in \mathbb{R}^{N \times B}$, a set of centroids $C \in \mathbb{R}^{K \times B}$ inferred by the KMeans algorithm and used for dividing the matrix in $K$ partitions, the inertia of the provided solution is defined as 
\begin{gather}
\label{inertia}
    \text{inertia}=
        \sum\limits_{j=1}^{K}
        \sum\limits_{i=1}^{N}
        \mid X_{ki} - C_k \mid ^ 2
\end{gather}
with $X_{ki}$ being the $i^{th}$ entry in $X$ assigned to the $k^{th}$ partition.

\section{Average Silhouette Score}
Given a matrix $X \in \mathbb{R}^{N \times B}$, divided in $k$ partitions by the KMeans algorithm, the average silhouette score of the provided solution is defined as 
\begin{gather}
\label{silhouette}
    \text{S}=
        \dfrac{1}{N}\sum\limits_{i=1}^{N}
        \dfrac{b_{ki} - a_{ki}}{max(a_{ki}, b_{ki})}
\end{gather}
with $a_{ki}$ being mean distance between $i^{th}$ entry in $X$ assigned to the $k^{th}$ partition and all other entries in $k$ and $b_{ki}$ the mean distance to the next nearest partition.

\section{Dropout Regularization}
Given parameters $\theta \in \mathbb{R}^z$ the dropout regularization is defined as 
\begin{gather}
    \label{dropout}
    dropout(\theta) = W \cdot diag(d)\\ \nonumber
    d = \{d_1, \dots, d_z\} \\ \nonumber
    d_i \sim Bernoulli(p) \\ \nonumber
\end{gather}
with $p$ being the probability of a parameter being dropped. 

\section{One Dimensional Spatial Dropout Regularization}
Given a sequence of weight matrices $W_{t_1:T} \in \mathbb{R}^{T \times z}$ the one dimensional spatial dropout regularization is defined as 
\begin{gather}
    \label{spatial_dropout}
    dropout(W) = W_{t:} \cdot d\\ \nonumber
    d = \mathit{I}^{z \times z} \cdot d_t \\ \nonumber
    d_t \sim Bernoulli(p) \\ \nonumber
\end{gather}
with $p$ being the probability of an entire step in the sequence of parameters being dropped. 

\section{Batch Normalization Regularization}
Given an embedding  $H \in \mathbb{R}^{N\times z}$ the batch normalization regularization is defined as 
\begin{gather}
    \label{batch_norm}
    BatchNorm(H) = \frac{h - \mu_b}{\sqrt{(\sigma_b)^2} + \epsilon}
\end{gather}
with $\mu_b$ and $\sigma_b$ being respectively the column-wise mean and standard deviation of the embedding $H$ and $\epsilon$ a small constant for avoiding division by zero.

\section{Ridge Regularization}
Given parameters $\theta$ the ridge regularization function $l2$ is defined as 
\begin{gather}
    \label{ridge}
    l2(\theta) = \lambda \sum_{n=1}^{N}\theta_n^2
\end{gather}
with $\lambda$ being a a constant controlling the strength of regularization.

\section{ElasticNet Regularization}
Given parameters $\theta$ the ridge regularization function $ElasticNet$ is defined as 
\begin{gather}
    \label{enet_reg}
    ElasticNet(\theta) = \lambda (\frac{1 - \alpha}{2}l2(\theta) + \alpha l1(\theta))
\end{gather}
with $\lambda$ and $\alpha$ being constants controlling respectively the global strength of regularization and the contribution of the $l1$ and $l2$ terms to the total ammount of regularization.

\section{Lasso Regularization}
Given parameters $\theta$ the ridge regularization function $l1$ is defined as 
\begin{gather}
    \label{lasso}
    l1(\theta) = \lambda \sum_{n=1}^{N}|\theta_n|
\end{gather}
with $\lambda$ being a a constant controlling the ammount of regularization.

\section{Fully Connected Operation}
Given an input matrix $X \in \mathbb{R}^{N \times h}$, the fully connected operation carried out by $L$ layers feedforward neural network can be defined as
\begin{gather}
    \label{fnn_operation}
    H_0 = X \\ \nonumber
    H_1 = \phi(W_1^\top H_0 + b_1)\\ \nonumber
   H_2 = \phi(W_2^\top H_2  + b_2)\\ \nonumber
    \dots\\ \nonumber
   H_L = \phi(W_L^\top H_{L-1}  + b_L) \nonumber
\end{gather}
with $\phi$ being a non-linear function, $\{W_1, \dots, W_N\}$ a set of learnable weights matrices of shape $W_l \in \mathbb{R}^{H_{l-1} \times H_{l}}$ and $\{b_1, \dots, b_N\}$ a set of learnable biases vectors of shape $b_l \in \mathbb{R}^{h_l}$.

\section{One-Hot Encode Operation}
Given an input set of numerical indices $X = \{1, 2, \dots, N\}$, the one-hot encode operation is defined as 
\begin{gather}
    \label{one_hot_encode_operation}
    1_X(x_i) = 
    \begin{cases}
        1,& \text{if } x_i \in X \\
        0,              & \text{otherwise}
    \end{cases}
\end{gather}

\section{Embedding Operation}
Given an input set of numerical indices $X = \{1, 2, \dots, N\}$, the embedding operation is defined as 
\begin{gather}
    \label{embedding_operation}
    h = \phi(W_{X,*})
\end{gather}
with $\phi$ being a non-linear function and $W$ an $\mathbb{R}^{N \times z}$ learnable weights matrix.

\section{LSTM Cell Operation}
Given an input set of multivariate time series $X_{t_1:T} \in \mathbb{R}^{N \times T \times B}$ with $N$ being the number of series, $T$ the length of the series and $B$ the dimensionality of the series. An LSTM cell operation, applied recursively along the temporal dimension for each $x_{*,t,*} \in X$, is defined as
\begin{gather}
    \label{lstm_operation}
    f_t = \sigma(W_{xf}^\top x_{*,t,*} + W_{hf}^\top H_{t-1} + b_f) \\ \nonumber
    i_t = \sigma(W_{xi}^\top x_{*,t,*} + W_{hi}^\top H_{t-1} + b_i) \\ \nonumber
    o_t = \tanh(W_{xo}^\top x_{*,t,*} + W_{ho}^\top H_{t-1} + b_o) \\ \nonumber
    \widehat{c}_t = \sigma(W_{xc}^\top x_{*,t,*} + W_{hc}^\top H_{t-1} + b_c) \\ \nonumber
    c_t = f_t \times c_{t-1} + i_t \times \widehat{c}_t \\ \nonumber
    H_t = o_t \times \tanh(c_t) \\ \nonumber
\end{gather}
with $\sigma$  being the sigmoid function, $\tanh$ the hyperbolic function, $W_{xf}$, $W_{xi}$, $W_{xo}$, $W_{xc}$ $\in \mathbb{R}^{B \times h}$ and $W_{hf}$,  $W_{hi}$,  $W_{ho}$, $W_{hc}$, $W_{hf}$ $\in \mathbb{R}^{h \times h}$ a set of learnable weights matrices, $b_f$, $b_i$, $b_o$, $b_c$ $\in \mathbb{R}^{h}$ , a set of learnable biases, $c_t$ the value at time $t$ of the conveyor belt matrix and $h_t$ the value at time $t$ of the hidden state matrix.


\section{Time Distributed Operation}
Given an input tensor $X \in \mathbb{R}^{N \times T \times h}$, a function $f$ with parameters $\theta$ can be applied along the temporal dimension for each $x_{*,t,*} \in X$ as
\begin{gather}
    \label{time_distributed}
    H_t = f(x_{*,t,*}; \theta)
\end{gather}
it has to be noted that this has the effect of sharing $\theta$ across all $t \in T$ but without temporal conditioning (e.g. like the LSTM operation presented in Appendix \ref{lstm_operation} does).

\section{One Dimensional Convolution Operation}
Given an input set of multivariate time series $X_{t_1:T} \in \mathbb{R}^{N \times T \times B}$ with $N$ being the number of series, $T$ the length of the series and $B$ the dimensionality of the series. A One Dimensional Convolution operation, applied recursively along the temporal dimension for each $x_{*,t,*} \in X$, is defined as
\begin{gather}
    \label{1d_conv}
    H_t = \phi(W_{K}^\top x_{*,t : t+k,*})
\end{gather}
with $\phi$ being a non-linear function and $K$ a $K \in \mathbb{R}^{k \times z}$ learnable weights matrix.

\section{One Dimensional Global Average Pooling Operation}
Given an input set of multivariate time series $X_{t_1:T} \in \mathbb{R}^{N \times T \times B}$ with $N$ being the number of series, $T$ the length of the series and $B$ the dimensionality of the series. A Global Average Pooling Operation, is defined as
\begin{gather}
    \label{1d_pool}
    H_t = \dfrac{1}{T} \sum_{t=1}^{t}X_{*,t,*}
\end{gather}

